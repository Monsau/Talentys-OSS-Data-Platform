# ğŸš€ INTÃ‰GRATION AIRBYTE - GUIDE COMPLET

**Date**: 15 octobre 2025  
**Objectif**: Ajouter Airbyte comme couche d'ingestion de donnÃ©es

---

## ğŸ¯ Pourquoi Airbyte ?

### Avantages
- âœ… **300+ connecteurs** (APIs, DBs, SaaS, Files)
- âœ… **Interface UI intuitive** (no-code configuration)
- âœ… **Sync incrÃ©mentales** (CDC, timestamps, cursors)
- âœ… **Orchestration native** avec scheduling
- âœ… **IntÃ©gration Airflow** (dÃ©jÃ  dans ta stack)
- âœ… **Stockage dans MinIO** (logs + data staging)
- âœ… **Open-source** (version communautaire gratuite)

### Position dans l'architecture

```
Sources externes        Airbyte           Destinations
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PostgreSQL    â”€â”€â”€â”                    â”Œâ”€â”€> MinIO (Parquet)
MySQL         â”€â”€â”€â”¤                    â”‚
MongoDB       â”€â”€â”€â”¤â”€â”€> [AIRBYTE] â”€â”€â”€â”€â”€â”€â”¼â”€â”€> PostgreSQL
APIs REST     â”€â”€â”€â”¤    (Ingestion)     â”‚
Google Sheets â”€â”€â”€â”¤                    â””â”€â”€> Elasticsearch
Salesforce    â”€â”€â”€â”˜                    
                                            â”‚
                                            â†“
                                        [DREMIO]
                                      (Query Engine)
                                            â”‚
                                            â†“
                                          [dbt]
                                       (Transform)
```

---

## ğŸ“¦ Installation

### Option 1: DÃ©marrage avec la stack existante

```powershell
# DÃ©marrer TOUT (stack principale + Airbyte)
docker-compose -f docker-compose.yml -f docker-compose-airbyte.yml up -d

# VÃ©rifier les services
docker ps | Select-String "airbyte"
```

### Option 2: DÃ©marrage sÃ©parÃ©

```powershell
# 1. DÃ©marrer la stack principale d'abord
docker-compose up -d

# 2. Attendre que tout soit prÃªt (90s)
Start-Sleep -Seconds 90

# 3. DÃ©marrer Airbyte
docker-compose -f docker-compose-airbyte.yml up -d

# 4. VÃ©rifier Airbyte
docker-compose -f docker-compose-airbyte.yml ps
```

---

## ğŸ”§ Configuration initiale

### 1. AccÃ©der Ã  l'interface Airbyte

**URL**: http://localhost:8000

**PremiÃ¨re connexion**:
- Pas de login requis (version OSS)
- Configuration automatique au premier accÃ¨s

### 2. CrÃ©er le bucket MinIO pour Airbyte

```powershell
# Via mc (MinIO Client)
docker exec dremio-minio mc alias set myminio http://localhost:9000 minioadmin minioadmin123
docker exec dremio-minio mc mb myminio/airbyte-logs
docker exec dremio-minio mc mb myminio/airbyte-staging

# VÃ©rifier
docker exec dremio-minio mc ls myminio/
```

Ou via l'interface MinIO:
- http://localhost:9001 (minioadmin / minioadmin123)
- CrÃ©er buckets: `airbyte-logs` et `airbyte-staging`

### 3. Initialiser la base de donnÃ©es Airbyte

```powershell
# Se connecter Ã  PostgreSQL
docker exec -it dremio-postgres psql -U postgres

# CrÃ©er la base Airbyte (si pas auto-crÃ©Ã©e)
CREATE DATABASE airbyte;
\q
```

---

## ğŸ“Š Cas d'usage: Connecteurs utiles

### ğŸ”¹ Connecteur 1: PostgreSQL â†’ MinIO (Parquet)

**Use case**: Exporter les donnÃ©es business vers le Data Lake

**Configuration**:
1. Source: PostgreSQL
   - Host: `dremio-postgres`
   - Port: `5432`
   - Database: `business_db`
   - User: `postgres`
   - Password: `postgres123`

2. Destination: S3 (MinIO)
   - Endpoint: `http://minio:9000`
   - Bucket: `airbyte-staging`
   - Access Key: `minioadmin`
   - Secret: `minioadmin123`
   - Format: Parquet
   - Path: `postgres-sync/`

3. Stream Configuration:
   - Tables: `customers`, `orders`
   - Sync mode: `Incremental - Append`
   - Cursor: `updated_at` (ou `id`)

### ğŸ”¹ Connecteur 2: API REST â†’ PostgreSQL

**Use case**: IngÃ©rer des donnÃ©es d'API externe

**Configuration**:
1. Source: Custom REST API
   - URL: `https://api.exemple.com/data`
   - Auth: Bearer Token
   - Pagination: Cursor-based

2. Destination: PostgreSQL
   - Host: `dremio-postgres`
   - Port: `5432`
   - Database: `business_db`
   - Schema: `external_data`

### ğŸ”¹ Connecteur 3: Google Sheets â†’ MinIO

**Use case**: IngÃ©rer des donnÃ©es manuelles

**Configuration**:
1. Source: Google Sheets
   - Spreadsheet URL
   - OAuth credentials

2. Destination: S3 (MinIO)
   - Bucket: `airbyte-staging`
   - Format: JSON ou CSV
   - Path: `google-sheets/`

---

## ğŸ”„ IntÃ©gration avec Airflow

### CrÃ©er un DAG Airflow pour orchestrer Airbyte

```python
# Fichier: airflow/dags/airbyte_sync_dag.py

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 15),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'airbyte_postgres_to_minio',
    default_args=default_args,
    description='Sync PostgreSQL to MinIO via Airbyte',
    schedule_interval='0 2 * * *',  # Daily at 2 AM
    catchup=False,
)

# Sync customers table
sync_customers = AirbyteTriggerSyncOperator(
    task_id='sync_customers',
    airbyte_conn_id='airbyte_connection',
    connection_id='<your-connection-id>',  # From Airbyte UI
    asynchronous=False,
    timeout=3600,
    wait_seconds=30,
    dag=dag,
)

# Sync orders table
sync_orders = AirbyteTriggerSyncOperator(
    task_id='sync_orders',
    airbyte_conn_id='airbyte_connection',
    connection_id='<your-connection-id>',
    asynchronous=False,
    timeout=3600,
    wait_seconds=30,
    dag=dag,
)

# Trigger dbt after sync
from airflow.operators.bash import BashOperator

run_dbt = BashOperator(
    task_id='run_dbt_models',
    bash_command='cd /path/to/dbt && dbt run',
    dag=dag,
)

# Orchestration
sync_customers >> sync_orders >> run_dbt
```

---

## ğŸ” Monitoring et logs

### 1. Logs Airbyte

**Dans l'UI**: http://localhost:8000 â†’ Jobs â†’ Logs

**Via Docker**:
```powershell
# Logs du worker
docker logs airbyte-worker -f

# Logs du serveur
docker logs airbyte-server -f

# Logs Temporal (workflow engine)
docker logs airbyte-temporal -f
```

### 2. Logs stockÃ©s dans MinIO

**Bucket**: `airbyte-logs`

**Structure**:
```
airbyte-logs/
â”œâ”€â”€ job-123/
â”‚   â”œâ”€â”€ attempt-1/
â”‚   â”‚   â”œâ”€â”€ logs.log
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â””â”€â”€ attempt-2/
â””â”€â”€ job-124/
```

**AccÃ¨s**:
- Via MinIO Console: http://localhost:9001
- Via mc: `docker exec dremio-minio mc ls myminio/airbyte-logs/`

### 3. MÃ©triques dans Airbyte UI

- **Records synced**: Nombre d'enregistrements
- **Bytes synced**: Volume de donnÃ©es
- **Duration**: Temps d'exÃ©cution
- **Status**: Success / Failed / Running

---

## ğŸ¯ ScÃ©nario complet d'intÃ©gration

### Objectif: Pipeline end-to-end avec Airbyte

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 1: INGESTION (Airbyte)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Source: PostgreSQL (business_db)                         â”‚
â”‚ â”œâ”€ customers (10,000 rows)                               â”‚
â”‚ â””â”€ orders (50,000 rows)                                  â”‚
â”‚                                                           â”‚
â”‚ Destination: MinIO (airbyte-staging)                     â”‚
â”‚ â”œâ”€ postgres-sync/customers/*.parquet                     â”‚
â”‚ â””â”€ postgres-sync/orders/*.parquet                        â”‚
â”‚                                                           â”‚
â”‚ FrÃ©quence: Toutes les 6 heures (incremental)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 2: CATALOGAGE (Dremio)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Source MinIO: airbyte-staging                            â”‚
â”‚ â”œâ”€ VDS: raw.airbyte_customers                            â”‚
â”‚ â””â”€ VDS: raw.airbyte_orders                               â”‚
â”‚                                                           â”‚
â”‚ SQL Query: SELECT * FROM raw.airbyte_customers           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 3: TRANSFORMATION (dbt)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Models:                                                  â”‚
â”‚ â”œâ”€ staging.stg_customers (clean + type)                 â”‚
â”‚ â”œâ”€ staging.stg_orders (clean + type)                    â”‚
â”‚ â””â”€ marts.customer_metrics (aggregation)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 4: METADATA (OpenMetadata)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Lineage:                                                 â”‚
â”‚ PostgreSQL â†’ Airbyte â†’ MinIO â†’ Dremio â†’ dbt             â”‚
â”‚                                                           â”‚
â”‚ Documentation auto-gÃ©nÃ©rÃ©e                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Script d'intÃ©gration automatique

CrÃ©ons un script pour tout configurer automatiquement :

```python
# scripts/setup_airbyte_integration.py

import requests
import time
import json

AIRBYTE_API = "http://localhost:8001/api/v1"

def wait_for_airbyte():
    """Wait for Airbyte to be ready"""
    print("Waiting for Airbyte...")
    for i in range(60):
        try:
            response = requests.get(f"{AIRBYTE_API}/health")
            if response.status_code == 200:
                print("âœ… Airbyte is ready!")
                return True
        except:
            pass
        time.sleep(2)
    return False

def create_workspace():
    """Create default workspace"""
    data = {
        "name": "Dremio Integration",
        "email": "admin@example.com"
    }
    response = requests.post(f"{AIRBYTE_API}/workspaces", json=data)
    workspace_id = response.json()["workspaceId"]
    print(f"âœ… Workspace created: {workspace_id}")
    return workspace_id

def create_postgres_source(workspace_id):
    """Create PostgreSQL source"""
    data = {
        "workspaceId": workspace_id,
        "name": "PostgreSQL BusinessDB",
        "sourceDefinitionId": "...",  # PostgreSQL connector ID
        "connectionConfiguration": {
            "host": "dremio-postgres",
            "port": 5432,
            "database": "business_db",
            "username": "postgres",
            "password": "postgres123",
            "ssl": False
        }
    }
    response = requests.post(f"{AIRBYTE_API}/sources", json=data)
    source_id = response.json()["sourceId"]
    print(f"âœ… PostgreSQL source created: {source_id}")
    return source_id

def create_minio_destination(workspace_id):
    """Create MinIO (S3) destination"""
    data = {
        "workspaceId": workspace_id,
        "name": "MinIO Data Lake",
        "destinationDefinitionId": "...",  # S3 connector ID
        "connectionConfiguration": {
            "s3_bucket_name": "airbyte-staging",
            "s3_bucket_path": "postgres-sync",
            "s3_bucket_region": "us-east-1",
            "s3_endpoint": "http://minio:9000",
            "access_key_id": "minioadmin",
            "secret_access_key": "minioadmin123",
            "format": {
                "format_type": "Parquet"
            }
        }
    }
    response = requests.post(f"{AIRBYTE_API}/destinations", json=data)
    dest_id = response.json()["destinationId"]
    print(f"âœ… MinIO destination created: {dest_id}")
    return dest_id

def create_connection(workspace_id, source_id, destination_id):
    """Create sync connection"""
    data = {
        "name": "PostgreSQL â†’ MinIO",
        "sourceId": source_id,
        "destinationId": destination_id,
        "scheduleType": "cron",
        "scheduleData": {
            "cron": {
                "cronExpression": "0 */6 * * *",  # Every 6 hours
                "cronTimeZone": "UTC"
            }
        },
        "syncCatalog": {
            "streams": [
                {
                    "stream": {"name": "customers"},
                    "config": {
                        "syncMode": "incremental",
                        "destinationSyncMode": "append",
                        "cursorField": ["id"]
                    }
                },
                {
                    "stream": {"name": "orders"},
                    "config": {
                        "syncMode": "incremental",
                        "destinationSyncMode": "append",
                        "cursorField": ["id"]
                    }
                }
            ]
        }
    }
    response = requests.post(f"{AIRBYTE_API}/connections", json=data)
    connection_id = response.json()["connectionId"]
    print(f"âœ… Connection created: {connection_id}")
    return connection_id

def main():
    print("=== AIRBYTE INTEGRATION SETUP ===\n")
    
    if not wait_for_airbyte():
        print("âŒ Airbyte not ready")
        return
    
    workspace_id = create_workspace()
    source_id = create_postgres_source(workspace_id)
    dest_id = create_minio_destination(workspace_id)
    connection_id = create_connection(workspace_id, source_id, dest_id)
    
    print("\n=== SETUP COMPLETE ===")
    print(f"Workspace: {workspace_id}")
    print(f"Source: {source_id}")
    print(f"Destination: {dest_id}")
    print(f"Connection: {connection_id}")
    print("\nâœ… Airbyte is configured!")
    print(f"UI: http://localhost:8000")

if __name__ == "__main__":
    main()
```

---

## ğŸš€ DÃ©marrage rapide

### Commandes rapides

```powershell
# 1. DÃ©marrer la stack complÃ¨te
docker-compose -f docker-compose.yml -f docker-compose-airbyte.yml up -d

# 2. Attendre 2 minutes
Start-Sleep -Seconds 120

# 3. VÃ©rifier que tout tourne
docker ps | Select-String "airbyte"

# 4. AccÃ©der Ã  l'UI
Start-Process "http://localhost:8000"

# 5. Configuration manuelle dans l'UI
#    - CrÃ©er Source (PostgreSQL)
#    - CrÃ©er Destination (S3/MinIO)
#    - CrÃ©er Connection avec sync schedule
```

---

## ğŸ“Š Ports utilisÃ©s

| Service | Port | Description |
|---------|------|-------------|
| Airbyte UI | 8000 | Interface web |
| Airbyte API | 8001 | API REST |
| Connector Builder | 8003 | Build custom connectors |
| Temporal | 7233 | Workflow engine (gRPC) |

---

## ğŸ¯ Prochaines Ã©tapes

1. âœ… DÃ©marrer Airbyte avec la stack
2. â³ Configurer premiÃ¨re source (PostgreSQL)
3. â³ Configurer destination (MinIO)
4. â³ CrÃ©er connexion avec sync
5. â³ Tester sync manuelle
6. â³ Configurer scheduling
7. â³ IntÃ©grer avec Airflow
8. â³ Ajouter dans OpenMetadata lineage

---

**Document crÃ©Ã© le**: 15 octobre 2025  
**Par**: AI Assistant  
**Status**: âœ… PRÃŠT POUR INTÃ‰GRATION
