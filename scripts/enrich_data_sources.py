#!/usr/bin/env python3
"""
Script pour enrichir les sources de donn√©es avant l'initialisation compl√®te
Alimente PostgreSQL et MinIO avec des donn√©es compl√®tes pour Dremio
"""

import os
import sys
import time
import logging
from pathlib import Path
import subprocess
import requests
import boto3
from botocore.exceptions import ClientError
import pandas as pd
import psycopg2
import psycopg2.extras

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataSourceEnricher:
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.postgres_config = {
            'host': 'localhost',
            'port': '5432',
            'database': 'business_db',
            'user': 'dbt_user',
            'password': 'dbt_password'
        }
        self.minio_config = {
            'endpoint': 'http://localhost:9000',
            'access_key': 'minio_admin',
            'secret_key': 'minio_password'
        }
        
    def wait_for_services(self):
        """Attendre que les services soient disponibles"""
        services = [
            ('PostgreSQL', 'localhost', 5432),
            ('MinIO', 'localhost', 9000),
            ('Dremio', 'localhost', 9047)
        ]
        
        for service_name, host, port in services:
            logger.info(f"V√©rification de {service_name}...")
            max_retries = 30
            for i in range(max_retries):
                try:
                    if service_name == 'PostgreSQL':
                        import socket
                        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                        result = sock.connect_ex((host, port))
                        sock.close()
                        if result == 0:
                            break
                    else:
                        response = requests.get(f"http://{host}:{port}", timeout=5)
                        if response.status_code in [200, 401, 403]:
                            break
                except:
                    pass
                
                if i == max_retries - 1:
                    raise Exception(f"{service_name} non disponible apr√®s {max_retries} tentatives")
                    
                logger.info(f"Attente de {service_name}... ({i+1}/{max_retries})")
                time.sleep(10)
            
            logger.info(f"‚úì {service_name} disponible")

    def enrich_postgresql(self):
        """Enrichir la base de donn√©es PostgreSQL avec des donn√©es compl√®tes"""
        logger.info("=== ENRICHISSEMENT POSTGRESQL ===")
        
        try:
            # Connexion √† PostgreSQL
            conn = psycopg2.connect(**self.postgres_config)
            cur = conn.cursor()
            
            # Ex√©cuter le script d'enrichissement
            sql_file = self.base_dir.parent / "postgres" / "init" / "02-enrich-business-data.sql"
            if sql_file.exists():
                logger.info(f"Ex√©cution du script: {sql_file}")
                with open(sql_file, 'r', encoding='utf-8') as f:
                    sql_content = f.read()
                
                # Ex√©cuter le script par blocs pour √©viter les erreurs
                statements = sql_content.split(';')
                for i, statement in enumerate(statements):
                    statement = statement.strip()
                    if statement and not statement.startswith('--'):
                        try:
                            cur.execute(statement)
                            conn.commit()
                        except Exception as e:
                            logger.warning(f"Erreur statement {i}: {e}")
                            conn.rollback()
                
                logger.info("‚úì Script PostgreSQL ex√©cut√© avec succ√®s")
            else:
                logger.error(f"Script non trouv√©: {sql_file}")
                
            # V√©rifier les donn√©es
            cur.execute("SELECT COUNT(*) FROM customers")
            customers_count = cur.fetchone()[0]
            
            cur.execute("SELECT COUNT(*) FROM orders")
            orders_count = cur.fetchone()[0]
            
            logger.info(f"‚úì Donn√©es PostgreSQL: {customers_count} clients, {orders_count} commandes")
            
            cur.close()
            conn.close()
            
        except Exception as e:
            logger.error(f"Erreur enrichissement PostgreSQL: {e}")
            raise

    def setup_minio_buckets(self):
        """Configurer les buckets MinIO et uploader les donn√©es"""
        logger.info("=== CONFIGURATION MINIO ===")
        
        try:
            # Connexion √† MinIO
            s3_client = boto3.client(
                's3',
                endpoint_url=self.minio_config['endpoint'],
                aws_access_key_id=self.minio_config['access_key'],
                aws_secret_access_key=self.minio_config['secret_key']
            )
            
            # Buckets √† cr√©er
            buckets = ['raw-data', 'staging-data', 'analytics-data']
            
            # Cr√©er les buckets
            for bucket in buckets:
                try:
                    s3_client.create_bucket(Bucket=bucket)
                    logger.info(f"‚úì Bucket cr√©√©: {bucket}")
                except ClientError as e:
                    if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':
                        logger.info(f"‚úì Bucket existe d√©j√†: {bucket}")
                    else:
                        logger.error(f"Erreur cr√©ation bucket {bucket}: {e}")
            
            # Uploader les fichiers de donn√©es
            sample_data_dir = self.base_dir / "minio" / "sample-data"
            if sample_data_dir.exists():
                files_to_upload = [
                    ('customers_external.csv', 'raw-data', 'external/customers/'),
                    ('transactions_2025.csv', 'raw-data', 'transactions/'),
                    ('products_catalog.csv', 'staging-data', 'products/'),
                    ('inventory_detailed.csv', 'staging-data', 'inventory/'),
                    ('analytics_summary.json', 'analytics-data', 'reports/')
                ]
                
                for filename, bucket, prefix in files_to_upload:
                    file_path = sample_data_dir / filename
                    if file_path.exists():
                        s3_key = f"{prefix}{filename}"
                        s3_client.upload_file(str(file_path), bucket, s3_key)
                        logger.info(f"‚úì Fichier upload√©: s3://{bucket}/{s3_key}")
                    else:
                        logger.warning(f"Fichier non trouv√©: {file_path}")
            
            logger.info("‚úì Configuration MinIO termin√©e")
            
        except Exception as e:
            logger.error(f"Erreur configuration MinIO: {e}")
            raise

    def verify_data_sources(self):
        """V√©rifier que les sources de donn√©es sont bien aliment√©es"""
        logger.info("=== VERIFICATION DES SOURCES ===")
        
        # V√©rifier PostgreSQL
        try:
            conn = psycopg2.connect(**self.postgres_config)
            cur = conn.cursor()
            
            tables_to_check = ['customers', 'orders', 'products', 'suppliers', 'categories']
            for table in tables_to_check:
                cur.execute(f"SELECT COUNT(*) FROM {table}")
                count = cur.fetchone()[0]
                logger.info(f"‚úì PostgreSQL {table}: {count} enregistrements")
            
            cur.close()
            conn.close()
            
        except Exception as e:
            logger.error(f"Erreur v√©rification PostgreSQL: {e}")
        
        # V√©rifier MinIO
        try:
            s3_client = boto3.client(
                's3',
                endpoint_url=self.minio_config['endpoint'],
                aws_access_key_id=self.minio_config['access_key'],
                aws_secret_access_key=self.minio_config['secret_key']
            )
            
            buckets = ['raw-data', 'staging-data', 'analytics-data']
            for bucket in buckets:
                response = s3_client.list_objects_v2(Bucket=bucket)
                count = response.get('KeyCount', 0)
                logger.info(f"‚úì MinIO {bucket}: {count} objets")
                
        except Exception as e:
            logger.error(f"Erreur v√©rification MinIO: {e}")

    def run(self):
        """Ex√©cuter l'enrichissement complet des sources de donn√©es"""
        logger.info("üöÄ DEMARRAGE DE L'ENRICHISSEMENT DES SOURCES DE DONNEES")
        
        try:
            # Attendre que les services soient disponibles
            self.wait_for_services()
            
            # Enrichir PostgreSQL
            self.enrich_postgresql()
            
            # Configurer MinIO
            self.setup_minio_buckets()
            
            # V√©rifier les donn√©es
            self.verify_data_sources()
            
            logger.info("‚úÖ ENRICHISSEMENT TERMINE AVEC SUCCES!")
            logger.info("Les sources de donn√©es sont maintenant compl√®tement aliment√©es.")
            logger.info("Vous pouvez maintenant ex√©cuter les scripts d'initialisation.")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå ERREUR LORS DE L'ENRICHISSEMENT: {e}")
            return False

if __name__ == "__main__":
    enricher = DataSourceEnricher()
    success = enricher.run()
    sys.exit(0 if success else 1)