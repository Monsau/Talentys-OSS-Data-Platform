# Airbyte é›†æˆæŒ‡å—

**ç‰ˆæœ¬**ï¼š3.2.0  
**æœ€åæ›´æ–°**ï¼š2025 å¹´ 10 æœˆ 16 æ—¥  
**è¯­è¨€**ï¼šæ³•è¯­

---

ï¼ƒï¼ƒ æ¦‚è¿°

Airbyte æ˜¯ä¸€ä¸ªå¼€æºæ•°æ®é›†æˆå¹³å°ï¼Œå¯ç®€åŒ–æ•°æ®ä»å„ç§æ¥æºåˆ°ç›®çš„åœ°çš„ç§»åŠ¨ã€‚æœ¬æŒ‡å—æ¶µç›–å°† Airbyte é›†æˆåˆ°æ•°æ®å¹³å°ã€é…ç½®è¿æ¥å™¨ä»¥åŠå»ºç«‹æ•°æ®ç®¡é“ã€‚

```mermaid
graph LR
    A[Sources de DonnÃ©es] -->|Extraire| B[Airbyte]
    B -->|Transformer| C[Normalisation Airbyte]
    C -->|Charger| D[Destinations]
    D --> E[MinIO S3]
    D --> F[PostgreSQL]
    D --> G[Dremio]
    
    style B fill:#615EFF
    style E fill:#C72E49
    style F fill:#336791
    style G fill:#FDB515
```

---

## Airbyte æ˜¯ä»€ä¹ˆï¼Ÿ

### ä¸»è¦ç‰¹ç‚¹

- **300+ é¢„æ„å»ºè¿æ¥å™¨**ï¼šAPIã€æ•°æ®åº“ã€æ–‡ä»¶ã€SaaS åº”ç”¨ç¨‹åº
- **å¼€æº**ï¼šè‡ªæ‰˜ç®¡ï¼Œå…·æœ‰å®Œå…¨çš„æ•°æ®æ§åˆ¶
- **å˜æ›´æ•°æ®æ•è·ï¼ˆCDCï¼‰**ï¼šå®æ—¶æ•°æ®åŒæ­¥
- **è‡ªå®šä¹‰è¿æ¥å™¨**ï¼šä½¿ç”¨ Python æˆ–ä½ä»£ç  CDK æ„å»ºè¿æ¥å™¨
- **æ•°æ®æ ‡å‡†åŒ–**ï¼šå°†åŸå§‹ JSON è½¬æ¢ä¸ºç»“æ„åŒ–è¡¨
- **ç›‘æ§å’Œè­¦æŠ¥**ï¼šè·Ÿè¸ªåŒæ­¥çŠ¶æ€å’Œæ•°æ®è´¨é‡

ï¼ƒï¼ƒï¼ƒ å»ºç­‘å­¦

```mermaid
graph TB
    subgraph "Plateforme Airbyte"
        W[Interface Web :8000]
        S[Serveur :8001]
        WK[Worker]
        T[Workflow Temporal]
        DB[(BD Airbyte)]
    end
    
    subgraph "Flux de DonnÃ©es"
        SRC[Sources] -->|Extraire| WK
        WK -->|DonnÃ©es Brutes| DEST[Destinations]
        WK -->|Normaliser| DBT[ModÃ¨les dbt]
        DBT --> DEST
    end
    
    W -->|Appels API| S
    S -->|Mettre en File| T
    T -->|ExÃ©cuter| WK
    WK -->|MÃ©tadonnÃ©es| DB
    
    style W fill:#615EFF
    style S fill:#615EFF
    style WK fill:#615EFF
```

---

ï¼ƒï¼ƒ è®¾æ–½

### å¿«é€Ÿå…¥é—¨

Airbyte åŒ…å«åœ¨è¯¥å¹³å°ä¸­ã€‚å¼€å§‹ï¼š

```bash
# DÃ©marrer services Airbyte
docker-compose -f docker-compose-airbyte.yml up -d

# VÃ©rifier statut
docker-compose -f docker-compose-airbyte.yml ps

# Voir logs
docker-compose -f docker-compose-airbyte.yml logs -f
```

### æœåŠ¡å·²å¯åŠ¨

|æœåŠ¡ |æ¸¯å£|æè¿° |
|--------|------|-------------|
| **airbyte-webapp** | 8000 |ç½‘é¡µç”¨æˆ·ç•Œé¢|
| **airbyte-æœåŠ¡å™¨** | 8001| APIæœåŠ¡å™¨|
| **airbyte-worker** | - |ä½œä¸šæ‰§è¡Œå¼•æ“|
| **airbyte-æ—¶é—´** | 7233 |å·¥ä½œæµç¨‹ç¼–æ’|
| **airbyte-db** | 5432 |å…ƒæ•°æ®æ•°æ®åº“ (PostgreSQL) |

### é¦–æ¬¡è®¿é—®

**ç½‘é¡µç•Œé¢ï¼š**
```
http://localhost:8000
```

**é»˜è®¤æ ‡è¯†ç¬¦ï¼š**
- **ç”µå­é‚®ä»¶**ï¼š`airbyte@example.com`
- **å¯†ç **ï¼š`password`

**é¦–æ¬¡ç™»å½•æ—¶è¯·æ›´æ”¹å¯†ç **ä»¥ç¡®ä¿å®‰å…¨ã€‚

---

ï¼ƒï¼ƒ é…ç½®

### é…ç½®å‘å¯¼

é¦–æ¬¡è®¿é—®æ—¶ï¼Œå®Œæˆé…ç½®å‘å¯¼ï¼š

1. **ç”µå­é‚®ä»¶é¦–é€‰é¡¹**ï¼šé…ç½®é€šçŸ¥
2. **Data Residency**ï¼šé€‰æ‹©æ•°æ®å­˜å‚¨ä½ç½®
3. **åŒ¿åä½¿ç”¨ç»Ÿè®¡**ï¼šæ¥å—/æ‹’ç»é¥æµ‹

### å·¥ä½œåŒºè®¾ç½®

å¯¼èˆªåˆ° **è®¾ç½® > å·¥ä½œåŒº**ï¼š

```yaml
Nom Workspace: Production Data Platform
ID Workspace: default
DÃ©finition Namespace: Destination Default
Format Namespace: ${SOURCE_NAMESPACE}
```

### èµ„æºé™åˆ¶

**æ–‡ä»¶**ï¼š`config/airbyte/config.yaml`

```yaml
# Allocation ressources par connecteur
resources:
  source:
    cpu_limit: "1.0"
    memory_limit: "1Gi"
    cpu_request: "0.25"
    memory_request: "256Mi"
  
  destination:
    cpu_limit: "1.0"
    memory_limit: "1Gi"
    cpu_request: "0.25"
    memory_request: "256Mi"
  
  orchestrator:
    cpu_limit: "0.5"
    memory_limit: "512Mi"
```

---

## è¿æ¥å™¨

### æºè¿æ¥å™¨

#### æº PostgreSQL

**ç”¨ä¾‹**ï¼šä»äº‹åŠ¡æ•°æ®åº“ä¸­æå–æ•°æ®

**é…ç½®ï¼š**

1. å¯¼èˆªè‡³ **æ¥æº > æ–°æ¥æº**
2. é€‰æ‹©**PostgreSQL**
3. é…ç½®è¿æ¥ï¼š

```yaml
Host: postgres
Port: 5432
Database: source_db
Username: readonly_user
Password: [MOT_DE_PASSE_SÃ‰CURISÃ‰]
SSL Mode: prefer

MÃ©thode RÃ©plication: Standard
  # Ou CDC pour changements temps rÃ©el:
  # MÃ©thode RÃ©plication: Logical Replication (CDC)
```

**æµ‹è¯•è¿æ¥** â†’ **è®¾ç½®æº**

#### REST API æº

**ç”¨ä¾‹**ï¼šä» API ä¸­æå–æ•°æ®

**é…ç½®ï¼š**

```yaml
Name: External API
URL Base: https://api.example.com/v1
Authentication:
  Type: Bearer Token
  Token: [API_TOKEN]

Endpoints:
  - name: customers
    path: /customers
    http_method: GET
    
  - name: orders
    path: /orders
    http_method: GET
    params:
      start_date: "{{ config['start_date'] }}"
```

#### æºæ–‡ä»¶ (CSV)

**ç”¨ä¾‹**ï¼šå¯¼å…¥ CSV æ–‡ä»¶

**é…ç½®ï¼š**

```yaml
Dataset Name: sales_data
URL: https://storage.example.com/sales.csv
Format: CSV
Provider:
  Storage: HTTPS
  User Provided Storage:
    URL: https://storage.example.com/sales.csv
```

#### å¸¸è§æ¥æº

|æ¥æº |ä½¿ç”¨æ¡ˆä¾‹ |ç–¾ç—…é¢„é˜²æ§åˆ¶ä¸­å¿ƒæ”¯æŒ|
|--------|--------|-------------|
| **PostgreSQL** |äº¤æ˜“æ¼«ç”»| âœ… æ˜¯çš„ |
| **MySQL** |äº¤æ˜“æ¼«ç”»| âœ… æ˜¯çš„ |
| **MongoDB** | NoSQL æ–‡æ¡£ | âœ… æ˜¯çš„ |
| **é”€å”®äººå‘˜** |å®¢æˆ·å…³ç³»ç®¡ç†æ•°æ®| âŒ å¦ |
| **è°·æ­Œè¡¨æ ¼** |ç”µå­è¡¨æ ¼| âŒ å¦ |
| **æ¡çº¹** |ä»˜æ¬¾æ•°æ®| âŒ å¦ |
| **REST API** |å®šåˆ¶ API | âŒ å¦ |
| **S3** |æ–‡ä»¶å­˜å‚¨ | âŒ å¦ |

### ç›®æ ‡è¿æ¥å™¨

#### MinIO S3 ç›®æ ‡

**ç”¨ä¾‹**ï¼šå°†åŸå§‹æ•°æ®å­˜å‚¨åœ¨æ•°æ®æ¹–ä¸­

**é…ç½®ï¼š**

1. å¯¼èˆªè‡³ **ç›®çš„åœ° > æ–°ç›®çš„åœ°**
2. é€‰æ‹©**S3**
3. é…ç½®è¿æ¥ï¼š

```yaml
S3 Bucket Name: datalake
S3 Bucket Path: airbyte-data/${NAMESPACE}/${STREAM_NAME}
S3 Bucket Region: us-east-1

# Point de terminaison MinIO
S3 Endpoint: http://minio:9000
Access Key ID: [MINIO_ROOT_USER]
Secret Access Key: [MINIO_ROOT_PASSWORD]

Output Format:
  Format Type: Parquet
  Compression: GZIP
  Block Size: 128MB
```

**æµ‹è¯•è¿æ¥** â†’ **è®¾ç½®ç›®çš„åœ°**

#### PostgreSQL ç›®æ ‡

**ç”¨ä¾‹**ï¼šåŠ è½½è½¬æ¢åçš„æ•°æ®è¿›è¡Œåˆ†æ

**é…ç½®ï¼š**

```yaml
Host: postgres
Port: 5432
Database: analytics_db
Username: analytics_user
Password: [MOT_DE_PASSE_SÃ‰CURISÃ‰]
Default Schema: public

Normalization:
  Mode: Basic
  # CrÃ©e tables normalisÃ©es depuis JSON imbriquÃ©
```

#### ç›®çš„åœ°å¾·é›·ç±³å¥¥

**ç”¨ä¾‹**ï¼šç›´æ¥åŠ è½½åˆ°æ•°æ®æ¹–å±‹

**é…ç½®ï¼š**

```yaml
Host: dremio
Port: 32010
Project: Production
Dataset: airbyte_data
Username: dremio_user
Password: [DREMIO_PASSWORD]

Connection Type: Arrow Flight
SSL: false
```

---

## è¿æ¥

### åˆ›å»ºè¿æ¥

è¿æ¥å°†æºé“¾æ¥åˆ°ç›®çš„åœ°ã€‚

```mermaid
sequenceDiagram
    participant S as Source
    participant A as Airbyte
    participant D as Destination
    
    S->>A: 1. Extraire donnÃ©es
    A->>A: 2. Appliquer transformations
    A->>D: 3. Charger donnÃ©es
    D->>D: 4. Normaliser (optionnel)
    A->>A: 5. Mettre Ã  jour Ã©tat
    
    Note over A: Sync terminÃ©e
```

#### ä¸€æ­¥ä¸€æ­¥

1. **å¯¼èˆªåˆ°è¿æ¥ > æ–°è¿æ¥**

2. **é€‰æ‹©æº**ï¼šé€‰æ‹©é…ç½®çš„æºï¼ˆä¾‹å¦‚ï¼šPostgreSQLï¼‰

3. **é€‰æ‹©ç›®çš„åœ°**ï¼šé€‰æ‹©ç›®çš„åœ°ï¼ˆä¾‹å¦‚ï¼šMinIO S3ï¼‰

4. **é…ç½®åŒæ­¥**ï¼š

```yaml
Nom Connexion: PostgreSQL â†’ MinIO
FrÃ©quence RÃ©plication: Every 24 hours
Namespace Destination: Custom
  Format Namespace: production_${SOURCE_NAMESPACE}

Streams:
  - customers
    Mode Sync: Full Refresh | Overwrite
    Champ Curseur: updated_at
    ClÃ© Primaire: customer_id
    
  - orders
    Mode Sync: Incremental | Append
    Champ Curseur: created_at
    ClÃ© Primaire: order_id
    
  - products
    Mode Sync: Full Refresh | Overwrite
    ClÃ© Primaire: product_id
```

5. **é…ç½®æ ‡å‡†åŒ–**ï¼ˆå¯é€‰ï¼‰ï¼š

```yaml
Normalization:
  Enable: true
  Option: Basic Normalization
  # Convertit JSON imbriquÃ© en tables plates
```

6. **æµ‹è¯•è¿æ¥** â†’ **è®¾ç½®è¿æ¥**

### åŒæ­¥æ¨¡å¼

|æ—¶å°š |æè¿° |ä½¿ç”¨æ¡ˆä¾‹ |
|------|-------------|-------------|
| **å…¨é¢åˆ·æ–°\|è¦†ç›–** |æ›¿æ¢æ‰€æœ‰æ•°æ®|å°ºå¯¸è¡¨ |
| **å…¨é¢åˆ·æ–°\|è¿½åŠ ** |æ·»åŠ æ‰€æœ‰è®°å½• |å†å²è¿½è¸ª|
| **å¢é‡\|è¿½åŠ ** |æ·»åŠ æ–°çš„/æ›´æ–°çš„è®°å½• |äº‹å®è¡¨|
| **å¢é‡\|é‡å¤æ•°æ®åˆ é™¤** |æ›´æ–°ç°æœ‰è®°å½• | SCD 1 å‹ |

### è§„åˆ’

**é¢‘ç‡é€‰é¡¹ï¼š**
- **æ‰‹åŠ¨**ï¼šæ‰‹åŠ¨è§¦å‘
- **æ¯å°æ—¶**ï¼šæ¯å°æ—¶
- **æ¯æ—¥**ï¼šæ¯ 24 å°æ—¶ä¸€æ¬¡ï¼ˆæŒ‡å®šæ—¶é—´ï¼‰
- **æ¯å‘¨**ï¼šä¸€å‘¨ä¸­çš„ç‰¹å®šæ—¥æœŸ
- **Cron**ï¼šè‡ªå®šä¹‰è°ƒåº¦ï¼ˆä¾‹å¦‚ï¼š`0 2 * * *`ï¼‰

**æ—¶é—´è¡¨ç¤ºä¾‹ï¼š**
```yaml
# Toutes les 6 heures
Cron: 0 */6 * * *

# Jours de semaine Ã  2h du matin
Cron: 0 2 * * 1-5

# Premier jour du mois
Cron: 0 0 1 * *
```

---

## æ•°æ®è½¬æ¢

### åŸºæœ¬æ ‡å‡†åŒ–

Airbyte åŒ…æ‹¬ä½¿ç”¨ dbt çš„**åŸºæœ¬æ ‡å‡†åŒ–**ï¼š

**å¥¹åšä»€ä¹ˆï¼š**
- å°†åµŒå¥— JSON è½¬æ¢ä¸ºå¹³é¢è¡¨
- åˆ›å»ºè¡¨`_airbyte_raw_*`ï¼ˆåŸå§‹ JSONï¼‰
- åˆ›å»ºæ ‡å‡†åŒ–ï¼ˆç»“æ„åŒ–ï¼‰è¡¨
- æ·»åŠ å…ƒæ•°æ®åˆ—ï¼ˆ`_airbyte_emitted_at`ã€`_airbyte_normalized_at`ï¼‰

**ä¾‹å­ï¼š**

**åŸå§‹ JSON** (`_airbyte_raw_customers`)ï¼š
```json
{
  "_airbyte_ab_id": "uuid-123",
  "_airbyte_emitted_at": "2025-10-16T10:00:00Z",
  "_airbyte_data": {
    "id": 1,
    "name": "Acme Corp",
    "contact": {
      "email": "info@acme.com",
      "phone": "+1234567890"
    },
    "addresses": [
      {"type": "billing", "city": "New York"},
      {"type": "shipping", "city": "Boston"}
    ]
  }
}
```

**æ ‡å‡†åŒ–è¡¨æ ¼ï¼š**

`customers`ï¼š
```sql
id | name | contact_email | contact_phone | _airbyte_normalized_at
1 | Acme Corp | info@acme.com | +1234567890 | 2025-10-16 10:05:00
```

`customers_addresses`ï¼š
```sql
_airbyte_customers_hashid | type | city
hash-123 | billing | New York
hash-123 | shipping | Boston
```

### è‡ªå®šä¹‰è½¬æ¢ (dbt)

å¯¹äºé«˜çº§è½¬æ¢ï¼Œè¯·ä½¿ç”¨ dbtï¼š

1. **ç¦ç”¨ Airbyte æ ‡å‡†åŒ–**
2. **åˆ›å»º dbt æ¨¡å‹** å¼•ç”¨è¡¨`_airbyte_raw_*`
3. **åŒæ­¥Airbyteåè¿è¡Œdbt**

**dbtæ¨¡å‹ç¤ºä¾‹ï¼š**
```sql
-- models/staging/stg_customers.sql
with source as (
    select * from {{ source('airbyte_raw', '_airbyte_raw_customers') }}
),

parsed as (
    select
        _airbyte_ab_id,
        _airbyte_emitted_at,
        (_airbyte_data->>'id')::int as customer_id,
        _airbyte_data->>'name' as customer_name,
        _airbyte_data->'contact'->>'email' as email,
        _airbyte_data->'contact'->>'phone' as phone
    from source
)

select * from parsed
```

---

## ç›‘æ§

### åŒæ­¥çŠ¶æ€

**ä»ªè¡¨æ¿ç½‘ç»œç•Œé¢ï¼š**
- **è¿æ¥**ï¼šæŸ¥çœ‹æ‰€æœ‰è¿æ¥
- **åŒæ­¥å†å²è®°å½•**ï¼šè¿‡å»çš„åŒæ­¥ä½œä¸š
- **åŒæ­¥æ—¥å¿—**ï¼šæ¯ä¸ªä½œä¸šçš„è¯¦ç»†æ—¥å¿—

**çŠ¶æ€æŒ‡ç¤ºå™¨ï¼š**
- ğŸŸ¢ **æˆåŠŸ**ï¼šåŒæ­¥æˆåŠŸå®Œæˆ
- ğŸ”´ **å¤±è´¥**ï¼šåŒæ­¥å¤±è´¥ï¼ˆæ£€æŸ¥æ—¥å¿—ï¼‰
- ğŸŸ¡ **æ­£åœ¨è¿è¡Œ**ï¼šåŒæ­¥æ­£åœ¨è¿›è¡Œä¸­
- âšª **å·²å–æ¶ˆ**ï¼šåŒæ­¥å·²è¢«ç”¨æˆ·å–æ¶ˆ

### æ—¥å¿—

**æŸ¥çœ‹åŒæ­¥æ—¥å¿—ï¼š**
```bash
# Logs serveur Airbyte
docker-compose -f docker-compose-airbyte.yml logs airbyte-server

# Logs worker (exÃ©cution sync rÃ©elle)
docker-compose -f docker-compose-airbyte.yml logs airbyte-worker

# Logs job spÃ©cifique
# Disponible dans Interface Web: Connections > [Connection] > Job History > [Job]
```

### æŒ‡æ ‡

**è¦ç›‘æ§çš„å…³é”®æŒ‡æ ‡ï¼š**
- **åŒæ­¥å½•éŸ³**ï¼šæ¯æ¬¡åŒæ­¥çš„å½•éŸ³æ•°é‡
- **åŒæ­¥å­—èŠ‚**ï¼šä¼ è¾“çš„æ•°æ®é‡
- **åŒæ­¥æŒç»­æ—¶é—´**ï¼šæ¯æ¬¡åŒæ­¥æ‰€éœ€çš„æ—¶é—´
- **å¤±è´¥ç‡**ï¼šåŒæ­¥å¤±è´¥çš„ç™¾åˆ†æ¯”

**å¯¼å‡ºæŒ‡æ ‡ï¼š**
```bash
# API Airbyte
curl -X GET "http://localhost:8001/api/v1/jobs/list" \
  -H "Content-Type: application/json" \
  -d '{
    "configTypes": ["sync"],
    "configId": "connection-id"
  }'
```

### è­¦æŠ¥

**åœ¨ **è®¾ç½® > é€šçŸ¥** ä¸­é…ç½®è­¦æŠ¥**ï¼š

```yaml
Type Notification: Slack
URL Webhook: https://hooks.slack.com/services/VOTRE/WEBHOOK/URL

Ã‰vÃ©nements:
  - Ã‰chec Sync
  - SuccÃ¨s Sync (optionnel)
  - Connexion DÃ©sactivÃ©e

Conditions:
  - Seuil Ã©chec: 3 Ã©checs consÃ©cutifs
```

---

## API ä½¿ç”¨

ï¼ƒï¼ƒï¼ƒ éªŒè¯

```bash
# Pas d'authentification requise pour localhost
# Pour production, configurez auth dans docker-compose-airbyte.yml
```

### å¸¸ç”¨APIè°ƒç”¨

#### åˆ—å‡ºæ¥æº

```bash
curl -X POST "http://localhost:8001/api/v1/sources/list" \
  -H "Content-Type: application/json" \
  -d '{
    "workspaceId": "workspace-id"
  }'
```

#### åˆ›å»ºè¿æ¥

```bash
curl -X POST "http://localhost:8001/api/v1/connections/create" \
  -H "Content-Type: application/json" \
  -d '{
    "sourceId": "source-id",
    "destinationId": "destination-id",
    "syncCatalog": {
      "streams": [
        {
          "stream": {
            "name": "customers",
            "jsonSchema": {...}
          },
          "config": {
            "syncMode": "incremental",
            "destinationSyncMode": "append",
            "cursorField": ["updated_at"]
          }
        }
      ]
    },
    "schedule": {
      "units": 24,
      "timeUnit": "hours"
    }
  }'
```

#### è§¦å‘åŒæ­¥

```bash
curl -X POST "http://localhost:8001/api/v1/connections/sync" \
  -H "Content-Type: application/json" \
  -d '{
    "connectionId": "connection-id"
  }'
```

#### è·å–å·¥ä½œçŠ¶æ€

```bash
curl -X POST "http://localhost:8001/api/v1/jobs/get" \
  -H "Content-Type: application/json" \
  -d '{
    "id": "job-id"
  }'
```

---

## ä¸ Dremio é›†æˆ

### å·¥ä½œæµç¨‹

```mermaid
sequenceDiagram
    participant Src as Source DonnÃ©es
    participant Ab as Airbyte
    participant S3 as MinIO S3
    participant Dr as Dremio
    participant Dbt as dbt
    
    Src->>Ab: 1. Extraire donnÃ©es
    Ab->>S3: 2. Charger vers S3 (Parquet)
    S3->>Dr: 3. Dremio lit S3
    Dr->>Dbt: 4. dbt transforme
    Dbt->>Dr: 5. Ã‰crire rÃ©sultats
    Dr->>Dr: 6. Couche requÃªte prÃªte
```

### é…ç½®æ­¥éª¤

1. **é…ç½® Airbyte å‘ MinIO S3 å……ç”µ**ï¼ˆè§ä¸Šæ–‡ï¼‰

2. **åœ¨Dremioä¸­æ·»åŠ S3æºï¼š**

```sql
-- Dans Interface Dremio: Sources > Add Source > S3
Nom Source: AirbyteData
Authentication: AWS Access Key
ClÃ© AccÃ¨s: [MINIO_ROOT_USER]
ClÃ© SecrÃ¨te: [MINIO_ROOT_PASSWORD]
Chemin Racine: /
PropriÃ©tÃ©s Connexion:
  fs.s3a.endpoint: minio:9000
  fs.s3a.path.style.access: true
  dremio.s3.compat: true
```

3. **åœ¨Dremioä¸­æŸ¥è¯¢Airbyteæ•°æ®ï¼š**

```sql
-- Parcourir structure S3
SELECT * FROM AirbyteData.datalake."airbyte-data"

-- RequÃªte table spÃ©cifique
SELECT *
FROM AirbyteData.datalake."airbyte-data".production_public.customers
LIMIT 100
```

4. **åˆ›å»º Dremio è™šæ‹Ÿæ•°æ®é›†ï¼š**

```sql
CREATE VDS airbyte_customers AS
SELECT
  id as customer_id,
  name as customer_name,
  contact_email as email,
  contact_phone as phone,
  _airbyte_emitted_at as last_updated
FROM AirbyteData.datalake."airbyte-data".production_public.customers
```

5. **ç”¨äº dbt å‹å·ï¼š**

```yaml
# dbt/models/sources.yml
sources:
  - name: airbyte
    schema: AirbyteData.datalake."airbyte-data".production_public
    tables:
      - name: customers
      - name: orders
      - name: products
```

---

## æœ€ä½³å®è·µ

ï¼ƒï¼ƒï¼ƒ è¡¨ç°

1. **å°½å¯èƒ½ä½¿ç”¨å¢é‡åŒæ­¥**
2. **åœ¨éé«˜å³°æ—¶æ®µå®‰æ’åŒæ­¥**
3. **ä½¿ç”¨ Parquet æ ¼å¼**ä»¥è·å¾—æ›´å¥½çš„å‹ç¼©æ•ˆæœ
4. **æŒ‰æ—¥æœŸå¯¹å¤§è¡¨è¿›è¡Œåˆ†åŒº**
5. **ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ**å¹¶è°ƒæ•´é™åˆ¶

### æ•°æ®è´¨é‡

1. **åœ¨æºè¿æ¥å™¨ä¸­å¯ç”¨æ•°æ®éªŒè¯**
2. **ä½¿ç”¨ä¸»é”®**æ¥æ£€æµ‹é‡å¤é¡¹
3. **é…ç½®åŒæ­¥å¤±è´¥è­¦æŠ¥**
4. **ç›‘æ§æ•°æ®æ–°é²œåº¦**æŒ‡æ ‡
5. **å¯¹åŸå§‹æ•°æ®å®æ–½ dbt æµ‹è¯•**

ï¼ƒï¼ƒï¼ƒ å®‰å…¨

1. **å¯¹æºä½¿ç”¨åªè¯»æ ‡è¯†ç¬¦**
2. **å°†æœºå¯†**å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ä¸­
3. **ä¸ºè¿æ¥å¯ç”¨ SSL/TLS**
4. **å®šæœŸæ›´æ–°æ‚¨çš„æ ‡è¯†ç¬¦**
5. **å®šæœŸå®¡æ ¸è®¿é—®æ—¥å¿—**

### æˆæœ¬ä¼˜åŒ–

1. **ä½¿ç”¨å‹ç¼©**ï¼ˆGZIPã€SNAPPYï¼‰
2. **ä»æºå¤´åˆ é™¤é‡å¤æ•°æ®**
3. **å°†æ—§æ•°æ®**å½’æ¡£è‡³å†·å­˜å‚¨
4. **ç›‘æ§åŒæ­¥é¢‘ç‡**ä¸è¦æ±‚
5. **æ¸…ç†å¤±è´¥çš„åŒæ­¥æ•°æ®**

---

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### åŒæ­¥å¤±è´¥ï¼šè¿æ¥è¶…æ—¶

**ç—‡çŠ¶ï¼š**
```
Failed to connect to source: Connection timeout
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# VÃ©rifier connectivitÃ© rÃ©seau
docker exec airbyte-worker ping postgres

# VÃ©rifier rÃ¨gles pare-feu
# VÃ©rifier hÃ´te/port source dans configuration
```

#### å†…å­˜ä¸è¶³é”™è¯¯

**ç—‡çŠ¶ï¼š**
```
OOMKilled: Container exceeded memory limit
```

**è§£å†³æ–¹æ¡ˆï¼š**
```yaml
# Augmenter limites mÃ©moire dans docker-compose-airbyte.yml
services:
  airbyte-worker:
    environment:
      - JOB_MAIN_CONTAINER_MEMORY_LIMIT=2Gi
      - JOB_MAIN_CONTAINER_MEMORY_REQUEST=1Gi
```

#### æ ‡å‡†åŒ–å¤±è´¥

**ç—‡çŠ¶ï¼š**
```
Normalization failed: dbt compilation error
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# VÃ©rifier logs dbt
docker-compose -f docker-compose-airbyte.yml logs airbyte-worker | grep dbt

# DÃ©sactiver normalisation et utiliser dbt personnalisÃ©
# Meilleur contrÃ´le sur logique transformation
```

#### åŒæ­¥æ€§èƒ½æ…¢

**è¯Šæ–­ï¼š**
```bash
# VÃ©rifier logs sync pour goulot d'Ã©tranglement
# Causes courantes:
# - Grand volume donnÃ©es
# - RequÃªte source lente
# - Latence rÃ©seau
# - Ressources insuffisantes
```

**è§£å†³æ–¹æ¡ˆï¼š**
- å¢åŠ å¢é‡åŒæ­¥é¢‘ç‡
- ä¸ºå…‰æ ‡å­—æ®µæ·»åŠ ç´¢å¼•
- ä½¿ç”¨ CDC è·å–å®æ—¶æº
- æ‰©å±•å·¥äººèµ„æº

---

## é«˜çº§ä¸»é¢˜

### è‡ªå®šä¹‰è¿æ¥å™¨

ä½¿ç”¨ Airbyte CDK æ„å»ºè‡ªå®šä¹‰è¿æ¥å™¨ï¼š

```bash
# Cloner modÃ¨le connecteur
git clone https://github.com/airbytehq/airbyte.git
cd airbyte/airbyte-integrations/connector-templates/python

# CrÃ©er nouveau connecteur
./create_connector.sh MyCustomAPI

# ImplÃ©menter logique connecteur
# Ã‰diter source.py, spec.yaml, schemas/

# Tester localement
python main.py check --config secrets/config.json
python main.py discover --config secrets/config.json
python main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json
```

### API ç¼–æ’

ä½¿ç”¨ Python è‡ªåŠ¨åŒ– Airbyteï¼š

```python
import requests

AIRBYTE_API = "http://localhost:8001/api/v1"

def trigger_sync(connection_id: str):
    """DÃ©clencher sync manuelle pour connexion"""
    response = requests.post(
        f"{AIRBYTE_API}/connections/sync",
        json={"connectionId": connection_id}
    )
    return response.json()

def get_sync_status(job_id: str):
    """VÃ©rifier statut job sync"""
    response = requests.post(
        f"{AIRBYTE_API}/jobs/get",
        json={"id": job_id}
    )
    return response.json()

# Utilisation
job = trigger_sync("my-connection-id")
status = get_sync_status(job["job"]["id"])
print(f"Statut sync: {status['job']['status']}")
```

---

ï¼ƒï¼ƒ èµ„æº

### æ–‡æ¡£

- **Airbyte æ–‡æ¡£**ï¼šhttps://docs.airbyte.com
- **è¿æ¥å™¨ç›®å½•**ï¼šhttps://docs.airbyte.com/integrations
- **API å‚è€ƒ**ï¼šhttps://airbyte-public-api-docs.s3.us-east-2.amazonaws.com/rapidoc-api-docs.html

ï¼ƒï¼ƒï¼ƒ ç¤¾åŒº

- **Slack**ï¼šhttps://slack.airbyte.io
- **GitHub**ï¼šhttps://github.com/airbytehq/airbyte
- **è®ºå›**ï¼šhttps://discuss.airbyte.io

---

## åç»­æ­¥éª¤

é…ç½® Airbyte åï¼š

1. **è®¾ç½® Dremio** - [Dremio è®¾ç½®æŒ‡å—](dremio-setup.md)
2. **åˆ›å»ºdbtæ¨¡å‹** - [dbtå¼€å‘æŒ‡å—](dbt-development.md)
3. **æ„å»ºä»ªè¡¨æ¿** - [Superset Dashboards æŒ‡å—](superset-dashboards.md)
4. **ç›‘æ§è´¨é‡** - [æ•°æ®è´¨é‡æŒ‡å—](data-quality.md)

---

**Airbyte é›†æˆæŒ‡å—ç‰ˆæœ¬**ï¼š3.2.0  
**æœ€åæ›´æ–°**ï¼š2025 å¹´ 10 æœˆ 16 æ—¥  
**ç»´æŠ¤è€…**ï¼šæ•°æ®å¹³å°å›¢é˜Ÿ