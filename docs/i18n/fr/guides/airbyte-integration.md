# Guide d'Int√©gration Airbyte

**Version**: 3.2.0  
**Derni√®re Mise √† Jour**: 16 octobre 2025  
**Langue**: Fran√ßais

---

## Vue d'ensemble

Airbyte est une plateforme d'int√©gration de donn√©es open source qui simplifie le d√©placement des donn√©es depuis diverses sources vers des destinations. Ce guide couvre l'int√©gration d'Airbyte dans la plateforme de donn√©es, la configuration des connecteurs et l'√©tablissement de pipelines de donn√©es.

```mermaid
graph LR
    A[Sources de Donn√©es] -->|Extraire| B[Airbyte]
    B -->|Transformer| C[Normalisation Airbyte]
    C -->|Charger| D[Destinations]
    D --> E[MinIO S3]
    D --> F[PostgreSQL]
    D --> G[Dremio]
    
    style B fill:#615EFF
    style E fill:#C72E49
    style F fill:#336791
    style G fill:#FDB515
```

---

## Qu'est-ce qu'Airbyte ?

### Fonctionnalit√©s Cl√©s

- **300+ Connecteurs Pr√©-construits**: APIs, bases de donn√©es, fichiers, applications SaaS
- **Open Source**: Auto-h√©berg√© avec contr√¥le total des donn√©es
- **Change Data Capture (CDC)**: Synchronisation de donn√©es en temps r√©el
- **Connecteurs Personnalis√©s**: Construire des connecteurs avec Python ou CDK low-code
- **Normalisation des Donn√©es**: Transformer JSON brut en tables structur√©es
- **Surveillance & Alertes**: Suivre le statut de synchronisation et la qualit√© des donn√©es

### Architecture

```mermaid
graph TB
    subgraph "Plateforme Airbyte"
        W[Interface Web :8000]
        S[Serveur :8001]
        WK[Worker]
        T[Workflow Temporal]
        DB[(BD Airbyte)]
    end
    
    subgraph "Flux de Donn√©es"
        SRC[Sources] -->|Extraire| WK
        WK -->|Donn√©es Brutes| DEST[Destinations]
        WK -->|Normaliser| DBT[Mod√®les dbt]
        DBT --> DEST
    end
    
    W -->|Appels API| S
    S -->|Mettre en File| T
    T -->|Ex√©cuter| WK
    WK -->|M√©tadonn√©es| DB
    
    style W fill:#615EFF
    style S fill:#615EFF
    style WK fill:#615EFF
```

---

## Installation

### D√©marrage Rapide

Airbyte est inclus dans la plateforme. D√©marrez-le avec :

```bash
# D√©marrer services Airbyte
docker-compose -f docker-compose-airbyte.yml up -d

# V√©rifier statut
docker-compose -f docker-compose-airbyte.yml ps

# Voir logs
docker-compose -f docker-compose-airbyte.yml logs -f
```

### Services D√©marr√©s

| Service | Port | Description |
|---------|------|-------------|
| **airbyte-webapp** | 8000 | Interface utilisateur web |
| **airbyte-server** | 8001 | Serveur API |
| **airbyte-worker** | - | Moteur d'ex√©cution de jobs |
| **airbyte-temporal** | 7233 | Orchestration workflow |
| **airbyte-db** | 5432 | Base de donn√©es m√©tadonn√©es (PostgreSQL) |

### Premier Acc√®s

**Interface Web:**
```
http://localhost:8000
```

**Identifiants par d√©faut:**
- **Email**: `airbyte@example.com`
- **Mot de passe**: `password`

**Changez le mot de passe** lors de la premi√®re connexion pour la s√©curit√©.

---

## Configuration

### Assistant de Configuration

Lors du premier acc√®s, compl√©tez l'assistant de configuration :

1. **Pr√©f√©rences Email**: Configurer les notifications
2. **R√©sidence des Donn√©es**: S√©lectionner l'emplacement de stockage des donn√©es
3. **Statistiques d'Usage Anonyme**: Accepter/refuser la t√©l√©m√©trie

### Param√®tres Workspace

Naviguez vers **Settings > Workspace**:

```yaml
Nom Workspace: Production Data Platform
ID Workspace: default
D√©finition Namespace: Destination Default
Format Namespace: ${SOURCE_NAMESPACE}
```

### Limites de Ressources

**Fichier**: `config/airbyte/config.yaml`

```yaml
# Allocation ressources par connecteur
resources:
  source:
    cpu_limit: "1.0"
    memory_limit: "1Gi"
    cpu_request: "0.25"
    memory_request: "256Mi"
  
  destination:
    cpu_limit: "1.0"
    memory_limit: "1Gi"
    cpu_request: "0.25"
    memory_request: "256Mi"
  
  orchestrator:
    cpu_limit: "0.5"
    memory_limit: "512Mi"
```

---

## Connecteurs

### Connecteurs Source

#### Source PostgreSQL

**Cas d'Usage**: Extraire donn√©es depuis base de donn√©es transactionnelle

**Configuration:**

1. Naviguez vers **Sources > New Source**
2. S√©lectionnez **PostgreSQL**
3. Configurez la connexion:

```yaml
Host: postgres
Port: 5432
Database: source_db
Username: readonly_user
Password: [MOT_DE_PASSE_S√âCURIS√â]
SSL Mode: prefer

M√©thode R√©plication: Standard
  # Ou CDC pour changements temps r√©el:
  # M√©thode R√©plication: Logical Replication (CDC)
```

**Test Connection** ‚Üí **Set up source**

#### Source API REST

**Cas d'Usage**: Extraire donn√©es depuis APIs

**Configuration:**

```yaml
Name: External API
URL Base: https://api.example.com/v1
Authentication:
  Type: Bearer Token
  Token: [API_TOKEN]

Endpoints:
  - name: customers
    path: /customers
    http_method: GET
    
  - name: orders
    path: /orders
    http_method: GET
    params:
      start_date: "{{ config['start_date'] }}"
```

#### Source Fichier (CSV)

**Cas d'Usage**: Importer fichiers CSV

**Configuration:**

```yaml
Dataset Name: sales_data
URL: https://storage.example.com/sales.csv
Format: CSV
Provider:
  Storage: HTTPS
  User Provided Storage:
    URL: https://storage.example.com/sales.csv
```

#### Sources Courantes

| Source | Cas d'Usage | Support CDC |
|--------|-------------|-------------|
| **PostgreSQL** | BD transactionnelle | ‚úÖ Oui |
| **MySQL** | BD transactionnelle | ‚úÖ Oui |
| **MongoDB** | Documents NoSQL | ‚úÖ Oui |
| **Salesforce** | Donn√©es CRM | ‚ùå Non |
| **Google Sheets** | Tableurs | ‚ùå Non |
| **Stripe** | Donn√©es paiement | ‚ùå Non |
| **API REST** | APIs personnalis√©es | ‚ùå Non |
| **S3** | Stockage fichiers | ‚ùå Non |

### Connecteurs Destination

#### Destination MinIO S3

**Cas d'Usage**: Stocker donn√©es brutes dans data lake

**Configuration:**

1. Naviguez vers **Destinations > New Destination**
2. S√©lectionnez **S3**
3. Configurez la connexion:

```yaml
S3 Bucket Name: datalake
S3 Bucket Path: airbyte-data/${NAMESPACE}/${STREAM_NAME}
S3 Bucket Region: us-east-1

# Point de terminaison MinIO
S3 Endpoint: http://minio:9000
Access Key ID: [MINIO_ROOT_USER]
Secret Access Key: [MINIO_ROOT_PASSWORD]

Output Format:
  Format Type: Parquet
  Compression: GZIP
  Block Size: 128MB
```

**Test Connection** ‚Üí **Set up destination**

#### Destination PostgreSQL

**Cas d'Usage**: Charger donn√©es transform√©es pour analytique

**Configuration:**

```yaml
Host: postgres
Port: 5432
Database: analytics_db
Username: analytics_user
Password: [MOT_DE_PASSE_S√âCURIS√â]
Default Schema: public

Normalization:
  Mode: Basic
  # Cr√©e tables normalis√©es depuis JSON imbriqu√©
```

#### Destination Dremio

**Cas d'Usage**: Chargement direct dans data lakehouse

**Configuration:**

```yaml
Host: dremio
Port: 32010
Project: Production
Dataset: airbyte_data
Username: dremio_user
Password: [DREMIO_PASSWORD]

Connection Type: Arrow Flight
SSL: false
```

---

## Connexions

### Cr√©er une Connexion

Une connexion lie une source √† une destination.

```mermaid
sequenceDiagram
    participant S as Source
    participant A as Airbyte
    participant D as Destination
    
    S->>A: 1. Extraire donn√©es
    A->>A: 2. Appliquer transformations
    A->>D: 3. Charger donn√©es
    D->>D: 4. Normaliser (optionnel)
    A->>A: 5. Mettre √† jour √©tat
    
    Note over A: Sync termin√©e
```

#### √âtape par √âtape

1. **Naviguez vers Connections > New Connection**

2. **S√©lectionnez Source**: Choisissez source configur√©e (ex: PostgreSQL)

3. **S√©lectionnez Destination**: Choisissez destination (ex: MinIO S3)

4. **Configurez Sync**:

```yaml
Nom Connexion: PostgreSQL ‚Üí MinIO
Fr√©quence R√©plication: Every 24 hours
Namespace Destination: Custom
  Format Namespace: production_${SOURCE_NAMESPACE}

Streams:
  - customers
    Mode Sync: Full Refresh | Overwrite
    Champ Curseur: updated_at
    Cl√© Primaire: customer_id
    
  - orders
    Mode Sync: Incremental | Append
    Champ Curseur: created_at
    Cl√© Primaire: order_id
    
  - products
    Mode Sync: Full Refresh | Overwrite
    Cl√© Primaire: product_id
```

5. **Configurez Normalisation** (optionnel):

```yaml
Normalization:
  Enable: true
  Option: Basic Normalization
  # Convertit JSON imbriqu√© en tables plates
```

6. **Test Connection** ‚Üí **Set up connection**

### Modes de Synchronisation

| Mode | Description | Cas d'Usage |
|------|-------------|-------------|
| **Full Refresh \| Overwrite** | Remplacer toutes les donn√©es | Tables de dimension |
| **Full Refresh \| Append** | Ajouter tous les enregistrements | Suivi historique |
| **Incremental \| Append** | Ajouter enregistrements nouveaux/mis √† jour | Tables de fait |
| **Incremental \| Deduped** | Mettre √† jour enregistrements existants | SCD Type 1 |

### Planification

**Options de Fr√©quence:**
- **Manual**: D√©clencher manuellement
- **Hourly**: Toutes les heures
- **Daily**: Toutes les 24 heures (sp√©cifier heure)
- **Weekly**: Jours sp√©cifiques de la semaine
- **Cron**: Planification personnalis√©e (ex: `0 2 * * *`)

**Exemples de Planifications:**
```yaml
# Toutes les 6 heures
Cron: 0 */6 * * *

# Jours de semaine √† 2h du matin
Cron: 0 2 * * 1-5

# Premier jour du mois
Cron: 0 0 1 * *
```

---

## Transformation de Donn√©es

### Normalisation Basique

Airbyte inclut **Basic Normalization** utilisant dbt:

**Ce qu'elle fait:**
- Convertit JSON imbriqu√© en tables plates
- Cr√©e tables `_airbyte_raw_*` (JSON brut)
- Cr√©e tables normalis√©es (structur√©es)
- Ajoute colonnes m√©tadonn√©es (`_airbyte_emitted_at`, `_airbyte_normalized_at`)

**Exemple:**

**JSON Brut** (`_airbyte_raw_customers`):
```json
{
  "_airbyte_ab_id": "uuid-123",
  "_airbyte_emitted_at": "2025-10-16T10:00:00Z",
  "_airbyte_data": {
    "id": 1,
    "name": "Acme Corp",
    "contact": {
      "email": "info@acme.com",
      "phone": "+1234567890"
    },
    "addresses": [
      {"type": "billing", "city": "New York"},
      {"type": "shipping", "city": "Boston"}
    ]
  }
}
```

**Tables Normalis√©es:**

`customers`:
```sql
id | name | contact_email | contact_phone | _airbyte_normalized_at
1 | Acme Corp | info@acme.com | +1234567890 | 2025-10-16 10:05:00
```

`customers_addresses`:
```sql
_airbyte_customers_hashid | type | city
hash-123 | billing | New York
hash-123 | shipping | Boston
```

### Transformations Personnalis√©es (dbt)

Pour transformations avanc√©es, utilisez dbt:

1. **D√©sactivez Normalisation Airbyte**
2. **Cr√©ez mod√®les dbt** r√©f√©ren√ßant tables `_airbyte_raw_*`
3. **Ex√©cutez dbt** apr√®s sync Airbyte

**Exemple de mod√®le dbt:**
```sql
-- models/staging/stg_customers.sql
with source as (
    select * from {{ source('airbyte_raw', '_airbyte_raw_customers') }}
),

parsed as (
    select
        _airbyte_ab_id,
        _airbyte_emitted_at,
        (_airbyte_data->>'id')::int as customer_id,
        _airbyte_data->>'name' as customer_name,
        _airbyte_data->'contact'->>'email' as email,
        _airbyte_data->'contact'->>'phone' as phone
    from source
)

select * from parsed
```

---

## Surveillance

### Statut de Synchronisation

**Tableau de Bord Interface Web:**
- **Connections**: Voir toutes les connexions
- **Historique Sync**: Jobs sync pass√©s
- **Logs Sync**: Logs d√©taill√©s par job

**Indicateurs de Statut:**
- üü¢ **Succeeded**: Sync termin√©e avec succ√®s
- üî¥ **Failed**: Sync √©chou√©e (v√©rifier logs)
- üü° **Running**: Sync en cours
- ‚ö™ **Cancelled**: Sync annul√©e par utilisateur

### Logs

**Voir logs sync:**
```bash
# Logs serveur Airbyte
docker-compose -f docker-compose-airbyte.yml logs airbyte-server

# Logs worker (ex√©cution sync r√©elle)
docker-compose -f docker-compose-airbyte.yml logs airbyte-worker

# Logs job sp√©cifique
# Disponible dans Interface Web: Connections > [Connection] > Job History > [Job]
```

### M√©triques

**M√©triques cl√©s √† surveiller:**
- **Enregistrements Synchronis√©s**: Nombre d'enregistrements par sync
- **Octets Synchronis√©s**: Volume de donn√©es transf√©r√©
- **Dur√©e Sync**: Temps pris par sync
- **Taux d'√âchec**: Pourcentage de syncs √©chou√©es

**Exporter m√©triques:**
```bash
# API Airbyte
curl -X GET "http://localhost:8001/api/v1/jobs/list" \
  -H "Content-Type: application/json" \
  -d '{
    "configTypes": ["sync"],
    "configId": "connection-id"
  }'
```

### Alertes

**Configurez alertes** dans **Settings > Notifications**:

```yaml
Type Notification: Slack
URL Webhook: https://hooks.slack.com/services/VOTRE/WEBHOOK/URL

√âv√©nements:
  - √âchec Sync
  - Succ√®s Sync (optionnel)
  - Connexion D√©sactiv√©e

Conditions:
  - Seuil √©chec: 3 √©checs cons√©cutifs
```

---

## Utilisation API

### Authentification

```bash
# Pas d'authentification requise pour localhost
# Pour production, configurez auth dans docker-compose-airbyte.yml
```

### Appels API Courants

#### Lister Sources

```bash
curl -X POST "http://localhost:8001/api/v1/sources/list" \
  -H "Content-Type: application/json" \
  -d '{
    "workspaceId": "workspace-id"
  }'
```

#### Cr√©er Connexion

```bash
curl -X POST "http://localhost:8001/api/v1/connections/create" \
  -H "Content-Type: application/json" \
  -d '{
    "sourceId": "source-id",
    "destinationId": "destination-id",
    "syncCatalog": {
      "streams": [
        {
          "stream": {
            "name": "customers",
            "jsonSchema": {...}
          },
          "config": {
            "syncMode": "incremental",
            "destinationSyncMode": "append",
            "cursorField": ["updated_at"]
          }
        }
      ]
    },
    "schedule": {
      "units": 24,
      "timeUnit": "hours"
    }
  }'
```

#### D√©clencher Sync

```bash
curl -X POST "http://localhost:8001/api/v1/connections/sync" \
  -H "Content-Type: application/json" \
  -d '{
    "connectionId": "connection-id"
  }'
```

#### Obtenir Statut Job

```bash
curl -X POST "http://localhost:8001/api/v1/jobs/get" \
  -H "Content-Type: application/json" \
  -d '{
    "id": "job-id"
  }'
```

---

## Int√©gration avec Dremio

### Workflow

```mermaid
sequenceDiagram
    participant Src as Source Donn√©es
    participant Ab as Airbyte
    participant S3 as MinIO S3
    participant Dr as Dremio
    participant Dbt as dbt
    
    Src->>Ab: 1. Extraire donn√©es
    Ab->>S3: 2. Charger vers S3 (Parquet)
    S3->>Dr: 3. Dremio lit S3
    Dr->>Dbt: 4. dbt transforme
    Dbt->>Dr: 5. √âcrire r√©sultats
    Dr->>Dr: 6. Couche requ√™te pr√™te
```

### √âtapes de Configuration

1. **Configurez Airbyte pour charger vers MinIO S3** (voir ci-dessus)

2. **Ajoutez source S3 dans Dremio:**

```sql
-- Dans Interface Dremio: Sources > Add Source > S3
Nom Source: AirbyteData
Authentication: AWS Access Key
Cl√© Acc√®s: [MINIO_ROOT_USER]
Cl√© Secr√®te: [MINIO_ROOT_PASSWORD]
Chemin Racine: /
Propri√©t√©s Connexion:
  fs.s3a.endpoint: minio:9000
  fs.s3a.path.style.access: true
  dremio.s3.compat: true
```

3. **Requ√™tez donn√©es Airbyte dans Dremio:**

```sql
-- Parcourir structure S3
SELECT * FROM AirbyteData.datalake."airbyte-data"

-- Requ√™te table sp√©cifique
SELECT *
FROM AirbyteData.datalake."airbyte-data".production_public.customers
LIMIT 100
```

4. **Cr√©ez Dataset Virtuel Dremio:**

```sql
CREATE VDS airbyte_customers AS
SELECT
  id as customer_id,
  name as customer_name,
  contact_email as email,
  contact_phone as phone,
  _airbyte_emitted_at as last_updated
FROM AirbyteData.datalake."airbyte-data".production_public.customers
```

5. **Utilisez dans mod√®les dbt:**

```yaml
# dbt/models/sources.yml
sources:
  - name: airbyte
    schema: AirbyteData.datalake."airbyte-data".production_public
    tables:
      - name: customers
      - name: orders
      - name: products
```

---

## Bonnes Pratiques

### Performance

1. **Utilisez Syncs Incr√©mentales** dans la mesure du possible
2. **Planifiez syncs pendant heures creuses**
3. **Utilisez format Parquet** pour meilleure compression
4. **Partitionnez grandes tables** par date
5. **Surveillez utilisation ressources** et ajustez limites

### Qualit√© des Donn√©es

1. **Activez validation donn√©es** dans connecteurs source
2. **Utilisez cl√©s primaires** pour d√©tecter doublons
3. **Configurez alertes** pour √©checs sync
4. **Surveillez m√©triques fra√Æcheur** donn√©es
5. **Impl√©mentez tests dbt** sur donn√©es brutes

### S√©curit√©

1. **Utilisez identifiants lecture seule** pour sources
2. **Stockez secrets** dans variables environnement
3. **Activez SSL/TLS** pour connexions
4. **Renouvelez identifiants** r√©guli√®rement
5. **Auditez logs d'acc√®s** p√©riodiquement

### Optimisation Co√ªts

1. **Utilisez compression** (GZIP, SNAPPY)
2. **D√©dupliquez donn√©es** √† la source
3. **Archivez anciennes donn√©es** vers stockage froid
4. **Surveillez fr√©quence sync** vs exigences
5. **Nettoyez donn√©es sync √©chou√©es**

---

## D√©pannage

### Probl√®mes Courants

#### √âchec Sync: Timeout Connexion

**Sympt√¥me:**
```
Failed to connect to source: Connection timeout
```

**Solution:**
```bash
# V√©rifier connectivit√© r√©seau
docker exec airbyte-worker ping postgres

# V√©rifier r√®gles pare-feu
# V√©rifier h√¥te/port source dans configuration
```

#### Erreur Manque de M√©moire

**Sympt√¥me:**
```
OOMKilled: Container exceeded memory limit
```

**Solution:**
```yaml
# Augmenter limites m√©moire dans docker-compose-airbyte.yml
services:
  airbyte-worker:
    environment:
      - JOB_MAIN_CONTAINER_MEMORY_LIMIT=2Gi
      - JOB_MAIN_CONTAINER_MEMORY_REQUEST=1Gi
```

#### √âchec Normalisation

**Sympt√¥me:**
```
Normalization failed: dbt compilation error
```

**Solution:**
```bash
# V√©rifier logs dbt
docker-compose -f docker-compose-airbyte.yml logs airbyte-worker | grep dbt

# D√©sactiver normalisation et utiliser dbt personnalis√©
# Meilleur contr√¥le sur logique transformation
```

#### Performance Sync Lente

**Diagnostic:**
```bash
# V√©rifier logs sync pour goulot d'√©tranglement
# Causes courantes:
# - Grand volume donn√©es
# - Requ√™te source lente
# - Latence r√©seau
# - Ressources insuffisantes
```

**Solutions:**
- Augmenter fr√©quence sync incr√©mentale
- Ajouter index sur champs curseur
- Utiliser CDC pour sources temps r√©el
- Mettre √† l'√©chelle ressources worker

---

## Sujets Avanc√©s

### Connecteurs Personnalis√©s

Construire connecteurs personnalis√©s avec Airbyte CDK:

```bash
# Cloner mod√®le connecteur
git clone https://github.com/airbytehq/airbyte.git
cd airbyte/airbyte-integrations/connector-templates/python

# Cr√©er nouveau connecteur
./create_connector.sh MyCustomAPI

# Impl√©menter logique connecteur
# √âditer source.py, spec.yaml, schemas/

# Tester localement
python main.py check --config secrets/config.json
python main.py discover --config secrets/config.json
python main.py read --config secrets/config.json --catalog integration_tests/configured_catalog.json
```

### Orchestration API

Automatiser Airbyte avec Python:

```python
import requests

AIRBYTE_API = "http://localhost:8001/api/v1"

def trigger_sync(connection_id: str):
    """D√©clencher sync manuelle pour connexion"""
    response = requests.post(
        f"{AIRBYTE_API}/connections/sync",
        json={"connectionId": connection_id}
    )
    return response.json()

def get_sync_status(job_id: str):
    """V√©rifier statut job sync"""
    response = requests.post(
        f"{AIRBYTE_API}/jobs/get",
        json={"id": job_id}
    )
    return response.json()

# Utilisation
job = trigger_sync("my-connection-id")
status = get_sync_status(job["job"]["id"])
print(f"Statut sync: {status['job']['status']}")
```

---

## Ressources

### Documentation

- **Docs Airbyte**: https://docs.airbyte.com
- **Catalogue Connecteurs**: https://docs.airbyte.com/integrations
- **R√©f√©rence API**: https://airbyte-public-api-docs.s3.us-east-2.amazonaws.com/rapidoc-api-docs.html

### Communaut√©

- **Slack**: https://slack.airbyte.io
- **GitHub**: https://github.com/airbytehq/airbyte
- **Forum**: https://discuss.airbyte.io

---

## Prochaines √âtapes

Apr√®s configuration d'Airbyte:

1. **Configurez Dremio** - [Guide Configuration Dremio](dremio-setup.md)
2. **Cr√©ez Mod√®les dbt** - [Guide D√©veloppement dbt](dbt-development.md)
3. **Construisez Tableaux de Bord** - [Guide Tableaux de Bord Superset](superset-dashboards.md)
4. **Surveillez Qualit√©** - [Guide Qualit√© Donn√©es](data-quality.md)

---

**Version Guide Int√©gration Airbyte**: 3.2.0  
**Derni√®re Mise √† Jour**: 16 octobre 2025  
**Maintenu Par**: √âquipe Plateforme Donn√©es
