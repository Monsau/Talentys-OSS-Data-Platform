# Comenzando con la plataforma de datos

**Versi√≥n**: 3.2.0  
**√öltima actualizaci√≥n**: 2025-10-16  
**Idioma**: Franc√©s

---

## Descripci√≥n general

Este tutorial lo gu√≠a a trav√©s de sus primeras interacciones con la plataforma de datos, desde la conexi√≥n a los servicios hasta la creaci√≥n de su primera canalizaci√≥n de datos con Airbyte, Dremio, dbt y Superset.

```mermaid
graph LR
    A[Acc√©der aux Services] --> B[Configurer Airbyte]
    B --> C[Connecter Dremio]
    C --> D[Cr√©er Mod√®les dbt]
    D --> E[Construire Tableau de Bord]
    E --> F[Pipeline Complet]
    
    style F fill:#90EE90
    style A fill:#87CEEB
```

**Tiempo estimado**: 60-90 minutos

---

## Requisitos previos

Antes de comenzar, aseg√∫rese de que:

- ‚úÖ Todos los servicios est√°n instalados y funcionando
- ‚úÖ Puedes acceder a interfaces web
- ‚úÖ El entorno virtual Python est√° habilitado
- ‚úÖ Conocimientos b√°sicos de SQL

**Compruebe que los servicios est√©n funcionando:**
```bash
docker-compose ps
docker-compose -f docker-compose-airbyte.yml ps
```

---

## Paso 1: Acceda a todos los servicios

### URL de servicio

| Servicios | URL | Credenciales predeterminadas |
|---------|----------|------------------------|
| **Airbyte** | http://localhost:8000 | airbyte@ejemplo.com / contrase√±a |
| **Dremio** | http://localhost:9047 | administrador/admin123 |
| **Superconjunto** | http://localhost:8088 | administrador / administrador |
| **MinIO** | http://localhost:9001 | minioadmin/minioadmin123 |

### Primera conexi√≥n

**Byte a√©reo:**
1. Abra http://localhost:8000
2. Complete el asistente de configuraci√≥n.
3. Establezca el nombre del espacio de trabajo: "Producci√≥n"
4. Anular las preferencias (es posible realizar una configuraci√≥n posterior)

**Dremio:**
1. Abra http://localhost:9047
2. Cree un usuario administrador en el primer acceso:
   - Nombre de usuario: `admin`
   - Correo electr√≥nico: `admin@example.com`
   - Contrase√±a: `admin123`
3. Haga clic en "Comenzar"

**Superconjunto:**
1. Abra http://localhost:8088
2. Inicie sesi√≥n con las credenciales predeterminadas
3. Cambiar contrase√±a: Configuraci√≥n ‚Üí Informaci√≥n de usuario ‚Üí Restablecer contrase√±a

---

## Paso 2: Configura tu primera fuente de datos en Airbyte

### Crear una fuente PostgreSQL

**Escenario**: extraer datos de una base de datos PostgreSQL.

1. **Navegue a Fuentes**
   - Haga clic en "Fuentes" en el men√∫ de la izquierda.
   - Haga clic en ‚Äú+ Nueva fuente‚Äù

2. **Seleccione PostgreSQL**
   - Busque "PostgreSQL"
   - Haga clic en el conector ‚ÄúPostgreSQL‚Äù

3. **Configurar conexi√≥n**
   ```yaml
   Source name: Production PostgreSQL
   Host: postgres
   Port: 5432
   Database: dremio_db
   Username: postgres
   Password: postgres123
   SSL Mode: prefer
   Replication Method: Standard
   ```

4. **Pruebe y ahorre**
   - Haga clic en "Configurar fuente"
   - Espere la prueba de conexi√≥n.
   - Fuente creada ‚úÖ

### Crear datos de muestra (opcional)

Si a√∫n no tiene ning√∫n dato, cree tablas de ejemplo:

```sql
-- Se connecter √† PostgreSQL
docker exec -it postgres psql -U postgres -d dremio_db

-- Cr√©er des tables exemples
CREATE TABLE customers (
    customer_id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(100),
    country VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE orders (
    order_id SERIAL PRIMARY KEY,
    customer_id INTEGER REFERENCES customers(customer_id),
    amount DECIMAL(10,2),
    status VARCHAR(20),
    order_date DATE DEFAULT CURRENT_DATE
);

-- Ins√©rer des donn√©es exemples
INSERT INTO customers (name, email, country) VALUES
    ('John Doe', 'john@example.com', 'USA'),
    ('Jane Smith', 'jane@example.com', 'UK'),
    ('Carlos Garcia', 'carlos@example.com', 'Spain'),
    ('Marie Dubois', 'marie@example.com', 'France'),
    ('Yuki Tanaka', 'yuki@example.com', 'Japan');

INSERT INTO orders (customer_id, amount, status) VALUES
    (1, 150.00, 'completed'),
    (1, 250.00, 'completed'),
    (2, 300.00, 'pending'),
    (3, 120.00, 'completed'),
    (4, 450.00, 'completed'),
    (5, 200.00, 'shipped');

-- V√©rifier les donn√©es
SELECT * FROM customers;
SELECT * FROM orders;
```

---

## Paso 3: Configurar el destino MinIO S3

### Crear un destino

1. **Navegar a Destinos**
   - Haga clic en "Destinos" en el men√∫ de la izquierda
   - Haga clic en ‚Äú+ Nuevo destino‚Äù

2. **Seleccione S3**
   - Busque "S3"
   - Haga clic en el conector ‚ÄúS3‚Äù

3. **Configurar MinIO como S3**
   ```yaml
   Destination name: MinIO Data Lake
   S3 Bucket Name: datalake
   S3 Bucket Path: raw-data
   S3 Bucket Region: us-east-1
   S3 Endpoint: http://minio:9000
   Access Key ID: minioadmin
   Secret Access Key: minioadmin123
   
   Output Format:
     Format Type: Parquet
     Compression: GZIP
     Block Size (Row Group Size): 128 MB
   ```

4. **Pruebe y ahorre**
   - Haga clic en "Configurar destino"
   - La prueba de conexi√≥n debe pasar ‚úÖ

---

## Paso 4: Crea tu primera conexi√≥n

### Vincular origen a destino

1. **Navegue a Conexiones**
   - Haga clic en "Conexiones" en el men√∫ de la izquierda
   - Haga clic en ‚Äú+ Nueva conexi√≥n‚Äù

2. **Seleccione fuente**
   - Elija "Producci√≥n PostgreSQL"
   - Haga clic en "Usar fuente existente"

3. **Seleccione Destino**
   - Elija "Lago de datos MinIO"
   - Haga clic en "Usar destino existente"

4. **Configurar sincronizaci√≥n**
   ```yaml
   Connection name: PostgreSQL ‚Üí MinIO
   Replication frequency: Every 24 hours at 02:00
   Destination Namespace: Custom format
     Format: production_${SOURCE_NAMESPACE}
   
   Streams to sync:
     ‚òë customers
       Sync mode: Full Refresh | Overwrite
       Primary key: customer_id
       Cursor field: created_at
       
     ‚òë orders
       Sync mode: Incremental | Append
       Primary key: order_id
       Cursor field: order_date
   ```

5. **Normalizaci√≥n**
   ```yaml
   Normalization: Disabled
   # Nous utiliserons dbt pour les transformations
   ```

6. **Copia de seguridad y sincronizaci√≥n**
   - Haga clic en "Configurar conexi√≥n"
   - Haga clic en "Sincronizar ahora" para ejecutar la primera sincronizaci√≥n
   - Monitorear el progreso de la sincronizaci√≥n

### Sincronizaci√≥n de monitores

```mermaid
sequenceDiagram
    participant PG as PostgreSQL
    participant AB as Airbyte Worker
    participant S3 as MinIO S3
    
    AB->>PG: 1. Extraction: SELECT * FROM customers
    PG->>AB: 2. Retour donn√©es (5 lignes)
    AB->>AB: 3. Transformation en Parquet
    AB->>S3: 4. T√©l√©versement vers datalake/raw-data/
    
    AB->>PG: 5. Extraction: SELECT * FROM orders WHERE order_date > last_sync
    PG->>AB: 6. Retour nouvelles donn√©es
    AB->>AB: 7. Transformation en Parquet
    AB->>S3: 8. T√©l√©versement vers datalake/raw-data/
    
    Note over AB: Synchronisation Termin√©e ‚úÖ
```

**Verificar estado de sincronizaci√≥n:**
- El estado debe mostrar "Exitoso" (verde)
- Registros sincronizados: ~11 (5 clientes + 6 pedidos)
- Ver registros para m√°s detalles

---

## Paso 5: Conecte Dremio a MinIO

### Agregar una fuente S3 en Dremio

1. **Navegue a Fuentes**
   - Abra http://localhost:9047
   - Haga clic en "Agregar fuente" (icono +)

2. **Seleccione S3**
   - Elija "Amazon S3"
   - Configurar como MinIO:

```yaml
General:
  Name: MinIOLake

Connection:
  Authentication: AWS Access Key
  AWS Access Key: minioadmin
  AWS Secret Key: minioadmin123
  
  Encrypt connection: No
  
Advanced Options:
  Connection Properties:
    fs.s3a.path.style.access: true
    fs.s3a.endpoint: minio:9000
    dremio.s3.compat: true
  
  Root Path: /
  
  Enable compatibility mode: Yes
```

3. **Pruebe y guarde**
   - Haga clic en "Guardar"
   - Dremio analizar√° los dep√≥sitos MinIO

### Explorar datos

1. **Navegue a la fuente de MinIOLake**
   - Desarrollar ‚ÄúMinIOLake‚Äù
   - Desarrollar el dep√≥sito "datalake"
   - Expandir la carpeta "datos sin procesar"
   - Ver la carpeta "production_public"

2. **Vista previa de datos**
   - Haga clic en la carpeta ‚Äúclientes‚Äù
   - Haga clic en el archivo Parquet
   - Haga clic en "Vista previa" para ver los datos
   - Los datos deben coincidir con PostgreSQL ‚úÖ

### Crear un conjunto de datos virtuales

1. **Datos de consulta**
   ```sql
   -- Dans Dremio SQL Runner
   SELECT *
   FROM MinIOLake.datalake."raw-data".production_public.customers
   LIMIT 100;
   ```

2. **Guardar como VDS**
   - Haga clic en "Guardar vista como"
   - Nombre: `vw_customers`
   - Espacio: `@admin` (tu espacio)
   - Haga clic en "Guardar"

3. **Formatear datos** (opcional)
   - Haga clic en `vw_customers`
   - Utilice la interfaz para cambiar el nombre de las columnas, cambiar tipos
   - Ejemplo: Cambiar el nombre de `customer_id` a `id`

---

## Paso 6: Crear plantillas dbt

### Inicializar el proyecto dbt

```bash
# Activer l'environnement virtuel
source venv/bin/activate  # Linux/macOS
# ou
.\venv\Scripts\activate  # Windows

# Naviguer vers le r√©pertoire dbt
cd dbt

# Tester la connexion
dbt debug

# Devrait afficher: "All checks passed!"
```

### Crear definici√≥n de fuente

**Archivo**: `dbt/models/sources.yml`

```yaml
version: 2

sources:
  - name: airbyte_raw
    description: Donn√©es brutes des synchronisations Airbyte
    database: MinIOLake.datalake."raw-data".production_public
    tables:
      - name: customers
        description: Donn√©es ma√Ætres clients
        columns:
          - name: customer_id
            description: Identifiant unique du client
            tests:
              - unique
              - not_null
          - name: email
            tests:
              - unique
              - not_null
      
      - name: orders
        description: Transactions de commandes
        columns:
          - name: order_id
            description: Identifiant unique de la commande
            tests:
              - unique
              - not_null
          - name: customer_id
            description: Cl√© √©trang√®re vers customers
            tests:
              - not_null
              - relationships:
                  to: source('airbyte_raw', 'customers')
                  field: customer_id
```

### Crear una plantilla de prueba

**Archivo**: `dbt/models/staging/stg_customers.sql`

```sql
-- Mod√®le de staging: Nettoyer et standardiser les donn√©es clients

{{ config(
    materialized='view',
    schema='staging'
) }}

with source as (
    select * from {{ source('airbyte_raw', 'customers') }}
),

cleaned as (
    select
        customer_id,
        trim(name) as customer_name,
        lower(trim(email)) as email,
        upper(trim(country)) as country_code,
        created_at,
        current_timestamp() as dbt_loaded_at
    from source
)

select * from cleaned
```

**Archivo**: `dbt/models/staging/stg_orders.sql`

```sql
-- Mod√®le de staging: Nettoyer et standardiser les donn√©es de commandes

{{ config(
    materialized='view',
    schema='staging'
) }}

with source as (
    select * from {{ source('airbyte_raw', 'orders') }}
),

cleaned as (
    select
        order_id,
        customer_id,
        amount,
        lower(trim(status)) as order_status,
        order_date,
        current_timestamp() as dbt_loaded_at
    from source
    where amount > 0  -- Filtre de qualit√© des donn√©es
)

select * from cleaned
```

### Crear una plantilla de centro comercial

**Archivo**: `dbt/models/marts/fct_customer_orders.sql`

```sql
-- Table de faits: R√©sum√© des commandes clients

{{ config(
    materialized='table',
    schema='marts'
) }}

with customers as (
    select * from {{ ref('stg_customers') }}
),

orders as (
    select * from {{ ref('stg_orders') }}
),

customer_metrics as (
    select
        customer_id,
        count(*) as total_orders,
        sum(amount) as total_spent,
        avg(amount) as avg_order_value,
        min(order_date) as first_order_date,
        max(order_date) as last_order_date,
        sum(case when order_status = 'completed' then 1 else 0 end) as completed_orders
    from orders
    group by customer_id
),

final as (
    select
        c.customer_id,
        c.customer_name,
        c.email,
        c.country_code,
        c.created_at as customer_since,
        
        coalesce(m.total_orders, 0) as total_orders,
        coalesce(m.total_spent, 0) as lifetime_value,
        coalesce(m.avg_order_value, 0) as avg_order_value,
        m.first_order_date,
        m.last_order_date,
        coalesce(m.completed_orders, 0) as completed_orders,
        
        datediff('day', m.last_order_date, current_date()) as days_since_last_order,
        
        case
            when m.total_orders >= 5 then 'VIP'
            when m.total_orders >= 2 then 'Regular'
            else 'New'
        end as customer_segment
        
    from customers c
    left join customer_metrics m on c.customer_id = m.customer_id
)

select * from final
```

### Ejecutar modelos dbt

```bash
# Ex√©cuter tous les mod√®les
dbt run

# Devrait afficher:
# Completed successfully
# Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3

# Ex√©cuter les tests
dbt test

# G√©n√©rer la documentation
dbt docs generate
dbt docs serve  # Ouvre le navigateur sur localhost:8080
```

### Registrarse en Dremio

```sql
-- V√©rifier les vues de staging
SELECT * FROM "@admin".staging.stg_customers;
SELECT * FROM "@admin".staging.stg_orders;

-- V√©rifier la table mart
SELECT * FROM "@admin".marts.fct_customer_orders
ORDER BY lifetime_value DESC;
```

---

## Paso 7: crear un panel en Superset

### Agregar base de datos Dremio

1. **Navegue a Bases de datos**
   - Abra http://localhost:8088
   - Haga clic en ‚ÄúDatos‚Äù ‚Üí ‚ÄúBases de datos‚Äù
   - Haga clic en ‚Äú+ Base de datos‚Äù

2. **Seleccione Dremio**
   ```yaml
   Database name: Dremio Lakehouse
   SQLAlchemy URI: dremio+flight://admin:admin123@dremio:32010
   
   Test connection: ‚úÖ Succ√®s
   ```

3. **Haga clic en "Conectar"**

### Crear un conjunto de datos

1. **Navegue a Conjuntos de datos**
   - Haga clic en "Datos" ‚Üí "Conjuntos de datos"
   - Haga clic en "+ Conjunto de datos"

2. **Configurar el conjunto de datos**
   ```yaml
   Database: Dremio Lakehouse
   Schema: @admin.marts
   Table: fct_customer_orders
   ```

3. **Haga clic en "Crear conjunto de datos y crear gr√°fico"**

### Crear gr√°ficos

#### Gr√°fico 1: Segmentos de clientes (Diagrama circular)

```yaml
Chart Type: Pie Chart
Datasource: fct_customer_orders

Dimensions:
  - customer_segment

Metrics:
  - COUNT(customer_id)

Filters: Aucun

Chart Options:
  Show Labels: Yes
  Show Legend: Yes
```

#### Gr√°fico 2: Ingresos por pa√≠s (Gr√°fico de barras)

```yaml
Chart Type: Bar Chart
Datasource: fct_customer_orders

Dimensions:
  - country_code

Metrics:
  - SUM(lifetime_value)

Sort by: SUM(lifetime_value) DESC
Limit: 10

Chart Options:
  Show Labels: Yes
  Color Scheme: Superset Colors
```

#### Gr√°fico 3: M√©tricas de clientes (gran cantidad)

```yaml
Chart Type: Big Number
Datasource: fct_customer_orders

Metric: COUNT(DISTINCT customer_id)
Subheader: Total Clients

Chart Options:
  Number Format: ,d
```

### Crear el panel

1. **Navegue a Paneles**
   - Haga clic en "Paneles"
   - Haga clic en "+ Panel de control"

2. **Configurar el panel**
   ```yaml
   Title: Analytique Clients
   Slug: customer-analytics
   Owners: admin
   Published: Yes
   ```

3. **Agregar gr√°ficos**
   - Arrastra y suelta los gr√°ficos creados.
   - Organizar en una cuadr√≠cula:
     ```
     [ Total Clients      ]
     [ Segments ] [ Revenu par Pays ]
     ```

4. **Agregar filtros** (opcional)
   - Haga clic en "Agregar filtro"
   - Filtrar por: c√≥digo_pa√≠s
   - Aplicar a todos los gr√°ficos.

5. **Guarde el panel**

---

## Paso 8: Verifique el proceso completo

### Pruebas de un extremo a otro

```mermaid
graph LR
    A[PostgreSQL<br/>Donn√©es Source] -->|Sync Airbyte| B[MinIO S3<br/>Donn√©es Brutes]
    B -->|Requ√™te Dremio| C[dbt<br/>Transformations]
    C -->|√âcriture| D[Dremio<br/>Marts]
    D -->|Requ√™te SQL| E[Superset<br/>Tableau de Bord]
    
    style A fill:#336791,color:#fff
    style B fill:#C72E49,color:#fff
    style C fill:#FF694B,color:#fff
    style D fill:#FDB515
    style E fill:#20A7C9,color:#fff
```

### Agregar nuevos datos

1. **Insertar nuevos registros en PostgreSQL**
   ```sql
   docker exec -it postgres psql -U postgres -d dremio_db
   
   INSERT INTO customers (name, email, country) VALUES
       ('Emma Wilson', 'emma@example.com', 'USA'),
       ('Li Wei', 'li@example.com', 'China');
   
   INSERT INTO orders (customer_id, amount, status) VALUES
       (6, 500.00, 'completed'),
       (7, 350.00, 'pending');
   ```

2. **Activar sincronizaci√≥n de Airbyte**
   - Abre la interfaz de Airbyte
   - Ir a la conexi√≥n "PostgreSQL ‚Üí MinIO"
   - Haga clic en "Sincronizar ahora"
   - Espera el final ‚úÖ

3. **Ejecute dbt**
   ```bash
   cd dbt
   dbt run
   ```

4. **Actualice el panel Superset**
   - Abre el tablero
   - Haga clic en el bot√≥n "Actualizar"
   - Deber√≠an aparecer nuevos datos ‚úÖ

### Verificar flujo de datos

```sql
-- Dans Dremio SQL Runner

-- 1. V√©rifier les donn√©es brutes d'Airbyte
SELECT COUNT(*) as raw_customers
FROM MinIOLake.datalake."raw-data".production_public.customers;
-- Devrait retourner: 7

-- 2. V√©rifier la vue de staging
SELECT COUNT(*) as staged_customers
FROM "@admin".staging.stg_customers;
-- Devrait retourner: 7

-- 3. V√©rifier la table mart
SELECT
    customer_segment,
    COUNT(*) as customers,
    SUM(lifetime_value) as total_revenue
FROM "@admin".marts.fct_customer_orders
GROUP BY customer_segment
ORDER BY total_revenue DESC;
```

---

## Paso 9: Automatizar la canalizaci√≥n

### Programar sincronizaci√≥n de Airbyte

Ya configurado para ejecutarse cada 24 horas a las 02:00.

Para editar:
1. Abrir conexi√≥n en Airbyte
2. Vaya a la pesta√±a "Configuraci√≥n"
3. Actualice "Frecuencia de replicaci√≥n"
4. Guardar

### Programar ejecuciones de dbt

**Opci√≥n 1: Trabajo cron (Linux)**
```bash
# √âditer crontab
crontab -e

# Ajouter ex√©cution dbt √† 2h30 quotidiennement (apr√®s sync Airbyte)
30 2 * * * cd /path/to/dremiodbt/dbt && /path/to/venv/bin/dbt run >> /var/log/dbt.log 2>&1
```

**Opci√≥n 2: secuencia de comandos Python**

**Archivo**: `scripts/run_pipeline.py`
```python
#!/usr/bin/env python3
"""
Ex√©cution automatis√©e du pipeline
Ex√©cute les mod√®les dbt apr√®s la synchronisation Airbyte
"""

import subprocess
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def run_dbt():
    """Ex√©cuter les mod√®les dbt"""
    dbt_dir = Path(__file__).parent.parent / 'dbt'
    
    logger.info("Ex√©cution des mod√®les dbt...")
    result = subprocess.run(
        ['dbt', 'run'],
        cwd=dbt_dir,
        capture_output=True,
        text=True
    )
    
    if result.returncode == 0:
        logger.info("Ex√©cution dbt termin√©e avec succ√®s")
        return True
    else:
        logger.error(f"√âchec ex√©cution dbt: {result.stderr}")
        return False

if __name__ == '__main__':
    success = run_dbt()
    exit(0 if success else 1)
```

### Programar con Docker Compose

**Archivo**: `docker-compose.scheduler.yml`
```yaml
version: '3.8'

services:
  dbt-scheduler:
    image: ghcr.io/dbt-labs/dbt-core:1.10.0
    volumes:
      - ./dbt:/usr/app/dbt
    command: >
      sh -c "while true; do
        dbt run --profiles-dir /usr/app/dbt;
        sleep 3600;
      done"
    networks:
      - dremio_network
```

---

## Pr√≥ximos pasos

¬°Felicidades! Ha creado una canalizaci√≥n de datos completa de un extremo a otro. üéâ

### M√°s informaci√≥n

1. **Airbyte Advanced** - [Gu√≠a de integraci√≥n de Airbyte](../guides/airbyte-integration.md)
2. **Optimizaci√≥n de Dremio** - [Gu√≠a de configuraci√≥n de Dremio](../guides/dremio-setup.md)
3. **Modelos dbt complejos** - [Gu√≠a de desarrollo de dbt](../guides/dbt-development.md)
4. **Paneles de control avanzados** - [Gu√≠a de paneles de control Superset](../guides/superset-dashboards.md)
5. **Calidad de datos** - [Gu√≠a de calidad de datos](../guides/data-quality.md)

### Soluci√≥n de problemas

Si tiene problemas, consulte:
- [Gu√≠a de soluci√≥n de problemas](../guides/troubleshooting.md)
- [Gu√≠a de instalaci√≥n](installation.md#troubleshooting)
- [Gu√≠a de configuraci√≥n] (configuration.md)

---

## Resumen

Has conseguido con √©xito:

- ‚úÖ Accede a los 7 servicios de la plataforma
- ‚úÖ Configurar una fuente Airbyte (PostgreSQL)
- ‚úÖ Configurar un destino Airbyte (MinIO S3)
- ‚úÖ Crea tu primera conexi√≥n Airbyte
- ‚úÖ Conecte Dremio a MinIO
- ‚úÖ Crear plantillas dbt (puesta en escena + marts)
- ‚úÖ Cree un panel de control de superconjunto
- ‚úÖ Verifique el flujo de datos de un extremo a otro
- ‚úÖ Automatizar la ejecuci√≥n del pipeline

**¬°Tu plataforma de datos ya est√° operativa!** üöÄ

---

**Versi√≥n de la Gu√≠a de Primeros Pasos**: 3.2.0  
**√öltima actualizaci√≥n**: 2025-10-16  
**Mantenido por**: Equipo de plataforma de datos